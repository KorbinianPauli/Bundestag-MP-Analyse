{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdp = \"https://www.fdpbt.de/fraktion/abgeordnete\"\n",
    "source_fdp = requests.get(fdp).text\n",
    "soup_fdp = BeautifulSoup(source_fdp, 'html.parser')\n",
    "\n",
    "cdu = \"https://www.cducsu.de/hier-stellt-die-cducsu-bundestagsfraktion-ihre-abgeordneten-vor\"\n",
    "source_cdu = requests.get(cdu).text\n",
    "soup_cdu = BeautifulSoup(source_cdu, 'html.parser')\n",
    "\n",
    "spd = \"https://www.spdfraktion.de/abgeordnete/alle?wp=19&view=list&old=19\"\n",
    "source_spd = requests.get(spd).text\n",
    "soup_spd = BeautifulSoup(source_spd, 'html.parser')\n",
    "\n",
    "# for Die Linke, one needs to extract the twitter info from each individual MdB website\n",
    "linke_base = \"https://www.linksfraktion.de/fraktion/abgeordnete/\"\n",
    "letters = [['a', 'e'], ['f', 'j'], ['k', 'o'], ['p', 't'], ['u', 'z']]\n",
    "linke_name_bins = []\n",
    "\n",
    "for letter in letters:\n",
    "    extension = f'{letter[0]}-bis-{letter[1]}/' \n",
    "    linke_name_bins.append(linke_base + extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_abg_fdp = soup_fdp.find(class_ = 'person-list').find_all(class_ = 'person-item-wrapper')\n",
    "all_abg_cdu = soup_cdu.find_all(class_ = 'teaser delegates')\n",
    "all_abg_spd = soup_spd.find_all(class_ = 'views-row')\n",
    " \n",
    "all_abg_linke = []\n",
    "for name_bin in linke_name_bins:\n",
    "    source = requests.get(name_bin).text\n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "    for abg in soup.find_all('div', attrs = {'class': 'col-xs-12 col-sm-12 col-md-6 col-lg-6'}):\n",
    "        extension = abg.find('h2').find('a')['href'].lstrip('/fraktion/abgeordnete/')\n",
    "        all_abg_linke.append(linke_base + extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_list = []\n",
    "for abg in all_abg_fdp:\n",
    "    name_field = abg.find(class_ = 'person-name')\n",
    "    funktion = name_field.find('span').text.strip()\n",
    "    name = name_field.text.strip('\\n').strip().rstrip(funktion).strip('\\n').strip()\n",
    "    twitter = abg.find('a', attrs = {'class': 'tw'}, href = True)\n",
    "    twitter_list.append(\n",
    "        {\n",
    "        'Partei': \"FDP\",\n",
    "        'Name': name,\n",
    "        'Twitter': twitter['href'] if twitter is not None else \"\"\n",
    "            }\n",
    "        )\n",
    "    \n",
    "for abg in all_abg_cdu:\n",
    "    twitter = abg.find(class_ = 'twitter')\n",
    "    twitter_list.append(\n",
    "        {\n",
    "        'Partei': \"CDU/CSU\",\n",
    "        'Name': abg.find('h2').find('span').text.strip(' '),\n",
    "        'Twitter': twitter.find('a', href = True)['href'] if twitter is not None else \"\"\n",
    "            }\n",
    "        )\n",
    "    \n",
    "for abg in all_abg_spd:\n",
    "    twitter = abg.find(class_ = 'ico_twitter')\n",
    "    twitter_list.append(\n",
    "        {\n",
    "        'Partei': \"SPD\",\n",
    "        'Name': abg.find('h3').find('a').get_text().strip(' '),\n",
    "        'Twitter': twitter['href'] if twitter is not None else \"\"\n",
    "            }\n",
    "        )\n",
    "    \n",
    "for abg in all_abg_linke:\n",
    "    abg_source = requests.get(abg).text\n",
    "    abg_soup = BeautifulSoup(abg_source, 'html.parser')\n",
    "    twitter = abg_soup.find('a', text = re.compile('Twitter-Profil'))\n",
    "    twitter_list.append(\n",
    "        {\n",
    "        'Partei': \"Die Linke\",\n",
    "        'Name': abg_soup.find('h1').text.strip(' '),\n",
    "        'Twitter': twitter['href'] if twitter is not None else \"\"\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_df = pd.DataFrame(twitter_list)\n",
    "# twitter_df['twitter'] = twitter_df['twitter'].apply(lambda x: x.lstrip('http://twitter.com/'))\n",
    "# twitter_df['twitter'] = twitter_df['twitter'].apply(lambda x: x.lstrip('https://twitter.com/'))\n",
    "# twitter_df['twitter'] = twitter_df['twitter'].apply(lambda x: x.strip(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abg_df.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_list = []\n",
    "for name in df['Name']:\n",
    "    interim = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", ' '.join(name.split(',')[::-1])).strip(' ') # placing first name before last name\n",
    "    interim = re.sub(r'(^\\w{1,6}\\. ?)', r'', interim) # stripping titles\n",
    "    interim = re.sub(r'(^\\w{1,6}\\. ?)', r'', interim)\n",
    "    interim = re.sub(r'(^\\w{1,6}\\. ?)', r'', interim)\n",
    "    interim = re.sub(r'(^\\w{1,6}\\. ?)', r'', interim)\n",
    "    interim = re.sub(r'(^\\w{1,6}\\. ?)', r'', interim)\n",
    "    interim = re.sub(r'(^\\w{1,6}\\. ?)', r'', interim)\n",
    "    interim = re.sub(r'(^\\w{1,6}\\. ?)', r'', interim)\n",
    "    interim = re.sub(r'(^\\w{1,6}\\. ?)', r'', interim)\n",
    "    interim = unidecode.unidecode(interim).strip(' ')\n",
    "    matching_list.append(re.sub(' +', ' ', interim))\n",
    "df['Name_matching'] = matching_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "afd_df = pd.read_csv('AFD.csv', encoding = \"ISO-8859-1\", delimiter = ';')\n",
    "afd_df.columns = ['Name', 'Partei', 'Twitter']\n",
    "columns_titles = ['Partei', 'Name', 'Twitter']\n",
    "afd_df=afd_df.reindex(columns=columns_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = pd.DataFrame(twitter_list)\n",
    "twitter_df = twitter_df.append(afd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_list_twitter = []\n",
    "for name in twitter_df['Name']:\n",
    "    interim = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", name).strip(' ')\n",
    "    interim = re.sub(r'(^\\w{1,6}\\. ?)', r'', interim)\n",
    "    interim = re.sub(r'(^\\w{1,6}\\. ?)', r'', interim)\n",
    "    interim = re.sub(r'(^\\w{1,6}\\. ?)', r'', interim)\n",
    "    interim = re.sub(r'(^\\w{1,6}\\. ?)', r'', interim)\n",
    "    interim = re.sub(r'(^\\w{1,6}\\. ?)', r'', interim)\n",
    "    interim = unidecode.unidecode(interim).strip(' ')\n",
    "    matching_list_twitter.append(re.sub(' +', ' ', interim))   \n",
    "twitter_df['Name_matching_control'] = matching_list_twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dataframes (df and twitter_df)\n",
    "df = pd.merge(df, twitter_df, how = 'left', left_on = 'Name_matching', right_on = 'Name_matching_control', suffixes = ('', '_right'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_matches = []\n",
    "# for i in range(len(df)):\n",
    "#     if (df['Partei'][i] != df['Partei_right'][i] and df['Partei'][i] in ('SPD', 'Die Linke', 'CDU/CSU', 'FDP', 'AFD')):\n",
    "#         non_matches.append(i)\n",
    "        \n",
    "# non_matches_twitter = []\n",
    "# for i in non_matches:\n",
    "#     non_match = df['Name_matching'][i].split()[-1]\n",
    "#     for name in twitter_df['Name_matching_control']:\n",
    "#         if non_match in name:\n",
    "#             non_matches_twitter.append([i, twitter_df.loc[twitter_df['Name_matching_control'] == name].index[0]])\n",
    "\n",
    "# for pair in non_matches_twitter:\n",
    "#     df['Name_matching'][i] = twitter_df['Name_matching_control'][j]\n",
    "#     df['Twitter'][i] = twitter_df['Twitter'][j]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding Twitter account into \"Soziale Medien\"-dictionary\n",
    "for i in range(len(df)):\n",
    "    df['Soziale Medien'][i]['Twitter'] = df['Twitter'][i]\n",
    "# dropping columns used for merging only\n",
    "df = df.drop(['Name_matching', 'Partei_right', 'Name_right', 'Twitter', 'Name_matching_control'], axis = 1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('abg_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

---
title: "4_1to3"
author: "Patrick Schulze, Simon Wiegrebe"
date: "June 2020"
output:
  pdf_document:
    toc: true
    number_sections: true
  html_document: default
bibliography: bibliography.bib
biblio-style: myabbrvnat
link-citations: yes
linkcolor: blue
header-includes:
  - \usepackage{caption}
  - \usepackage{float}
  - \captionsetup{labelformat=empty}
  - \usepackage{multirow}
  - \usepackage{graphicx}
  - \usepackage{booktabs}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Preparation, include=FALSE}
# ----------------------------------------------------------------------------------------------
# ---------------------------------------- Preparation -----------------------------------------
# ----------------------------------------------------------------------------------------------

# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
  "betareg", "ggcorrplot", "grid", "gridExtra", "huge", "knitr", "mvtnorm", 
  "quanteda", "reshape2", "scales", "stm", "stringi", "tidyverse", "tm"
)
not_installed <- packages_required[!packages_required %in%
                                     installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
  lapply(
    not_installed,
    install.packages,
    repos = "http://cran.us.r-project.org",
    dependencies = TRUE,
    type = itype
  )
}
lapply(packages_required, library, character.only = TRUE)

# set working directory
# setwd('C:\\Users\\Simon\\OneDrive\\Uni\\LMU\\SS 2020\\Statistisches Consulting\\Bundestag-MP-Analyse')
# setwd("/Users/patrickschulze/Desktop/Consulting/Bundestag-MP-Analyse")
```

# Results

```{r include=FALSE}
# load data
data <- readRDS("../data/preprocessed_monthly.rds")
colnames_table <- read.csv(file = "../data/topic_monthly_colnames.csv")
data_corpus <- readRDS("../data/prep_monthly.rds")

# data_aggregated <- readRDS("../data/preprocessed.rds") # MP-level (non-monthly) data
```

## Hyperparameter Search and Model Fitting

```{r include=FALSE}
# # search hyperparameter space for optimal K
# hyperparameter_search <- searchK(
#   documents = data$documents,
#   vocab = data$vocab,
#   data = data$meta,
#   # K = c(5,6,7,8,9,10),
#   K = c(5,10,15,20,25,30,35,40),
#   prevalence =~ Partei,
#   max.em.its = 150,
#   init.type = "Spectral"
# )

# load searchK results
searchK_data <- readRDS("../data/searchK_large_data.rds")
```

Throughout this topic analysis we use the *stm* package, which is implemented in the R programming language (@stm). The most important hyperparameter choice when fitting an STM is the number of topics, $K$. While there is no *true* or *optimal* number of topics, we explore the hyperparameter space using the *searchK* function to get an understanding of the impact of $K$ on model fit. We use four of the metrics that come with this function, *held-out likelihood*, *semantic coherence*, *exclusivity*, and *residuals*.

The *held-out likelihood* approach is based on document completion. The *searchK* function randomly holds out a proportion of some of the documents; both the number of documents from which a portion is held out and the respective held-out proportions can be specified by the user. This gives rise to a set of held-out words, for which the likelihood is calculated, given the trained model. Thus, the higher this held-out likelihood, the more predictive power the model has on average. For more detailed information on held-out likelihood based on document completion and other types of held-out likelihoods, see @wallach2009evaluation.

Regarding the second metric, first introduced by @mimno2011optimizing, a model with $K$ topics is *semantically coherent* whenever those words that characterize a specific topic $k$ (i.e., the most frequent words within topic $k$) also do appear in the same documents. In order to formally define semantic coherence, let first $D(v)$ be the *document frequency* of word $v$ (that is, the number of documents where $v$ occurs at least once) and let $D(v, v')$ be the *co-document frequency* of words $v$ and $v'$ (that is, the number of documents where both $v$ *and* $v'$ occur at least once). Furthermore, consider the $M$ most probable words in a given topic $k$. Then, semantic coherence for topic $k$, $C_{k}$, is defined as follows:

\begin{align*}
C_{k} &= \sum_{i=2}^{M}\sum_{j=2}^{i-1}\log(\frac{D(v_{i}, v_{j})+1}{D(v_{j})}).
\end{align*}

That is, semantic coherence is the sum of (logarithmized) proportions of word co-occurrences to total word occurrences, the additive factor $1$ in the numerator just being a smoothness adjustment. It becomes apparent that by having some words that are very frequent across a couple of documents, we could achieve high semantic coherence without our topics being semantically coherent at all once we look beyond those common words (@stm, @mimno2011optimizing). As a partial remedy, we previously excluded some of these overly frequent words (see section 3.2).

A natural "counter-metric" of semantic coherence is *exclusivity*, which basically tells us to which degree a topic's words *only* occur in that topic. To be specific, for a given word $v\in{V}$, the empirical frequencies of $v$ in topic $k$, $\beta_{k,v}$, are normalized across all topics $k\in\{1,...,K\}$. This way, these normalized frequencies now represent the probability of observing topic $k$, conditional upon the word being $v$ - that is, the exclusivity of word $v$ regarding topic $k$. Formally, exclusivity of word $v$ to topic $k$, $E_{k,v}$, is thus defined as:

\begin{align*}
E_{k,v} &= \beta_{k,v}/\sum_{j=1}^{K}\beta_{j,v}.
\end{align*}

Combining a word's frequency and exclusivity finally yields its Frequency-Exclusivity (*FREX*) score, explained in more detail in section 4.2 below (@bischof2012summarizing).

Finally, *residuals* is a metric based on residual dispersion. Recall that $z_{d,n}$ is drawn from a $K$-category multinomial distribution, which is a member of the exponential family. Therefore, its dispersion parameter is equal to one, according to theory. This way, an observed residual dispersion larger than one roughly indicates that the number of topics $K$ was most likely chosen insufficiently small. See @taddy2012estimation for a detailed derivation.

Another aspect to be taken into account when choosing $K$ (or, to be precise, when choosing a search grid for searchK) is interpretability. While a large K certainly allows for a more fine-grained determination of topics, the resulting topics might be rather difficult to label. Furthermore, for large K we would obtain many topics which could be considered sub-topics of the topics we would obtain when using a smaller value for K. The graph below shows the four metrics, as introduced above, for values of K between 5 and 40 (in steps of 5).

```{r echo=FALSE}
plot_heldout <- ggplot(data = searchK_data$results, aes(x = K, y = heldout)) +
                  geom_line() +
                  geom_point() +
                  labs(y = "held-out likelihood") +
                  theme_minimal() +
                  theme(axis.text.x = element_text(vjust = 1, size = 12, hjust = 1),
                        axis.title.x = element_text(size = 12, face = "bold"),
                        axis.title.y = element_text(size = 12, face = "bold"))

plot_semcoh <- ggplot(data = searchK_data$results, aes(x = K, y = semcoh)) + 
                  geom_line() +
                  geom_point() +
                  labs(y = "semantic coherence") +
                  theme_minimal() +
                  theme(axis.text.x = element_text(vjust = 1, size = 12, hjust = 1),
                        axis.title.x = element_text(size = 12, face = "bold"),
                        axis.title.y = element_text(size = 12, face = "bold"))

plot_exclus <- ggplot(data = searchK_data$results, aes(x = K, y = exclus)) +
                  geom_line() +
                  geom_point() +
                  labs(y = "exclusivity") +
                  theme_minimal() +
                  theme(axis.text.x = element_text(vjust = 1, size = 12, hjust = 1),
                        axis.title.x = element_text(size = 12, face = "bold"),
                        axis.title.y = element_text(size = 12, face = "bold"))

plot_residual <- ggplot(data = searchK_data$results, aes(x = K, y = residual)) + 
                  geom_line() +
                  geom_point() +
                  labs(y = "residuals") +
                  theme_minimal() +
                  theme(axis.text.x = element_text(vjust = 1, size = 12, hjust = 1),
                        axis.title.x = element_text(size = 12, face = "bold"),
                        axis.title.y = element_text(size = 12, face = "bold"))

gridExtra::grid.arrange(plot_heldout, plot_semcoh, plot_exclus, plot_residual, ncol=2)

K <- 15
```

Both 15 and 20 topics seem to be good trade-offs between the metrics used. As mentioned above, no true or optimal K exists. Taking into account the interpretability aspect, we opt for $K$ = `r K`. For comparison, we also conducted the subsequent analysis for a small (and easily interpretable) number of topics, $K$ = 6, as well as for $K$ = 20. In general, topics generated are similar, but for $K$ = 6 only around three are clear-cut, while for $K$ = 20 some topics could easily be grouped together. This further corroborates our choice that $K$ = `r K` indeed seems to be a good trade-off. 

Before fitting the model, we need to choose the document-level covariates we want to include. Since a topic model is explorative by definition, we simply include those covariates that seem to be most influential *a priori*: party and state (both categorical), date (as smooth effect), as well as percentage of immigrants, GDP per capita, unemployment rate, and the 2017 election results of the MP's respective party (the last four as smooth effects and on an electoral-district level).

```{r include=FALSE}
# choose covariates and number of topics
covar <- "Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) + 
  s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)"
content_var <- "Partei"
outcome <- ""

prevalence <- as.formula(paste(outcome, covar, sep = "~")) 

# fit model
# mod_prev <- stm::stm(
#   documents = data$documents,
#   vocab = data$vocab,
#   data = data$meta,
#   K = K,
#   prevalence = prevalence,
#   gamma.prior = 'L1',
#   seed = 123,
#   max.em.its = 200,
#   init.type = "Spectral")
# saveRDS(mod_prev, "../data/mod_prev_monthly.rds")
 
mod_prev <- readRDS("../data/mod_prev_monthly.rds")
```

## Labelling

As a first step after fitting the model, we would like to visually inspect the resulting topics, in particular their most representative words. However, representativeness of words for a given topic depends on the weighting metric used. The STM comes with four topic-word metrics - *highest probability*, *FREX*, *Lift*, and *Score* - which are discussed in the following.

Given a topic $k$, *highest probability* simply outputs those words in the topic-specific word vector $\beta_{k}$ with the highest corpus frequency, i.e, those with the highest absolute frequency across all documents. Using the same notation as in section 4.1 above, let $\beta_{k,v}$ again be the (empirical) frequency of word $v$ within topic $k$. Then the highest probability word within topic $k$ is simply $\underset{v \in V}{\operatorname{argmax\beta_{k,v}}}$. This relatively simple measure only takes into account how often words occur in absolute terms, but not how specific those words are to the given topic. This is why we observe words like *wichtig*, *berlin*, or *frag* within the highest probability words for several topics. And since such words are very common, unspecific words, they are not particularly useful for distinguishing or labelling topics.

To also account for the degree to which a word *exclusively* belongs to a certain topic, we also consider the top words according to the *FREX* metric. It takes into account not only how frequent but also how exclusive words are. Formally, the FREX score of word $v$ with respect to topic $k$ is calculated as follows:

\begin{align*}
FREX_{k,v} &= (\frac{\omega}{ECDF(\beta_{k,v}/\sum_{j=1}^{K}\beta_{j,v})} + \frac{1-\omega}{ECDF(\beta_{k,v})})^{-1} &= (\frac{\omega}{ECDF(E_{k,v})} + \frac{1-\omega}{ECDF(\beta_{k,v})})^{-1},
\end{align*}

where $\omega$ is the weight  assigned to exclusivity (set to 0.7 by default in the STM), $E_{k,v}$ is the word's exclusivity as defined in section 4.1, and *ECDF* is the empirical CDF. Thus, for a given topic, $FREX_{k,v}$ is simply the harmonic mean of i) the rank of word $v$ by probability within topic $k$ (frequency rank) and ii) the rank of topic $k$ by the frequency of word $v$, across all topics $j \in \{1,...,K\}$ (exclusivity rank). Further information on the estimation of *FREX* can be found in @stm and in @bischof2012summarizing.

*Lift* is another topic-word metric, where the frequency of word $v$ within topic $k$,  $\beta_{k,v}$, is weighted by the inverse of $v$'s relative frequency across the entire corpus, i.e., $v$'s empirical corpus probability. Formally:

\begin{align*}
Lift_{k,v} &= \beta_{k,v}/(\omega_{v}/\sum_{v}\omega_{v}).
\end{align*}

where $\omega_{v}$ denotes the word count of word $v$ in the entire corpus. This way, Lift gives higher weight to those words that rarely appear in other topics. Further information on Lift can be found in @taddy2012estimation.

Finally, the *Score* metric for word $v$ and topic $k$ is formally defined as:

\begin{align*}
Score_{k,v} &= \beta_{k,v}(\log\beta_{k,v} - 1/K\sum_{j}^{K}\log\beta_{j,v})
\end{align*}

Thus, Score weights word $v$'s frequency within topic $k$, $\beta_{k,v}$, by the difference between $v$'s log frequency within topic $k$ and the average of $v$'s log frequencies across all $K$ topics. This can roughly (but not exactly) be seen as: $\beta_{k,v}$ is weighted by the proportion of $v$'s log frequency within topic $k$ to $v$'s average logarithmic frequency across all topics. For further information on the Score metric, see the R package *lda* (@chang2010package).

```{r echo=FALSE}
# labeling workflow (for each topic): 

## (1) inspect most frequent words per topic (using different metrics as well as word cloud)
## (2) evaluate most representative documents per topic
## (3) assign label

# first, prepare objects/variables needed for labelling process

## table of MAP topic proportions per document (for all topics)
topic_props <- make.dt(
  mod_prev, 
  data$meta[c("Name", "Partei", "Datum", "Bundesland")]) %>% 
  cbind(docname = names(data$documents), .)

## top words per topic (for all topics)
n <- 5
topic_words <- labelTopics(mod_prev, n = n)

## topic to be evaluated
topic_number <- 1
topic_number_long <- paste0("Topic", topic_number)

## number of top documents to be printed in step (2)
docs_number <- 10

## initialize list with empty labels
topic_labels <- list(
  Topic1 = NULL,
  Topic2 = NULL,
  Topic3 = NULL,
  Topic4 = NULL,
  Topic5 = NULL,
  Topic6 = NULL,
  Topic7 = NULL,
  Topic8 = NULL,
  Topic9 = NULL,
  Topic10 = NULL,
  Topic11 = NULL,
  Topic12 = NULL,
  Topic13 = NULL,
  Topic14 = NULL,
  Topic15 = NULL
)
```

To get a broad overview of which words characterize each one of the topics, the output below shows, for each topic $k$, the `r n` top words according to each of the four topic-word evaluation metrics.

```{r echo=FALSE}
# top words for all topics
topic_words
```

A key task of topic analysis is to actually ascribe a meaning to the topics identified, i.e., labelling them. While this is clearly where human judgment should and does come into play, we attempt to conduct the labelling in a more stratetic (and thus less subjective) manner, following a 3-step procedure. This procedure is exemplified using topic `r topic_number`.

First, we consider the *words* contained in the topic, for instance by simply inspecting the top words (see output above). For a better visualization, we use a word cloud. As shown below, for a given topic (i.e., conditional upon a specific topic being chosen), it shows words weighted by their frequency. For instance, by judging at first sight topic `r topic_number` appears to be about right-wing nationalist issues, particularly immigration.

```{r echo=FALSE}
# word cloud
cloud(mod_prev, topic = topic_number, scale = c(2.0, 0.25))
```

Second, to get a more thorough insight into the topic, we take a look into actual *documents*, specifically into those showing the highest proportion for topic `r topic_number`.

```{r include=FALSE}
# actual labelling porcess

## (1) inspect most frequent words per topic
# cloud(mod_prev, topic = topic_number, scale = c(2.5, 0.25)) # word cloud
# topic_words$prob[topic_number,] # 20 most frequent words
# logbeta_matrix <- mod_prev$beta$logbeta[[1]]
# mod_prev$vocab[which.max(logbeta_matrix[topic_number,])] # most frequent word directly from (log)beta vector

## (2) evaluate most representative documents per topic
data_corpus$docname <- paste0(data_corpus$Twitter_Username, "_", data_corpus$Jahr, "_", data_corpus$Monat)

repr_docs <-  topic_props %>%
  arrange(desc(!!as.symbol(topic_number_long))) %>%
  .[1:docs_number, c("Name", "docname", "Datum", "Partei", "Bundesland", topic_number_long)] %>%
  left_join(data_corpus[,c("Tweets_Dokument", "docname")], 
            by = "docname")
```

For instance, the most representative document for topic `r topic_number`, with a proportion of `r scales::percent(repr_docs[topic_number_long][1,1], accuracy = 0.01)` is the one by MP `r repr_docs$Name[1]`, a member of the `r repr_docs$Partei[1]` party from `r repr_docs$Bundesland[1]`, from `r repr_docs$Datum[1]` which starts with:

```{r echo=FALSE, null_x, results = "asis"}
substr(repr_docs$Tweets_Dokument[1], 0, 256) # view most representative document
```

The second most representative document, still for topic `r topic_number`, has a proportion of `r scales::percent(repr_docs[topic_number_long][2,1], accuracy = 0.01)`. Its author is the same as for the first document, `r repr_docs$Name[2]`, but the date now is `r repr_docs$Datum[2]`. The document starts with:

```{r echo=FALSE, results = "asis"}
substr(repr_docs$Tweets_Dokument[2], 0, 254) # view 2nd most representative document
```

```{r echo=FALSE}
## (3) assign label
topic_labels[[topic_number]] <- "right/nationalist"
```

The documents confirm the first impression gained through top words and the word cloud: `r topic_number` concerns right-wing nationalist issues, in particular immigration. Thus, as a third step, we finally label the topic: in this case, as `r topic_labels[topic_number]`.

We repeat this 3-step procedure (inspecting top words and word cloud, reading through top documents, assigning a 1- or 2-word label) for all remaining topics, arriving at the following manual labels:.

```{r echo=FALSE}
## (3) assign label
topic_labels <- list(
  Topic1 = "Right/Nationalist",
  Topic2 = "Miscellaneous 1",
  Topic3 = "Climate Economics",
  Topic4 = "Social/Housing",
  Topic5 = "Digital/Future",
  Topic6 = "Climate Protection",
  Topic7 = "Europe",
  Topic8 = "Corona",
  Topic9 = "Left/Anti-war",
  Topic10 = "Twitter/Politics 1",
  Topic11 = "Twitter/Politics 2",
  Topic12 = "Miscellaneous 2",
  Topic13 = "Twitter/Politics 3",
  Topic14 = "Right-wing Extremism",
  Topic15 = "Society/Solidarity"
)
```

```{r echo=FALSE}
topic_labels %>%
  matrix(dimnames = list(names(topic_labels))) %>%
  knitr::kable()
```

## Global-level Topic Analysis

```{r include=FALSE}
doc_lengths <- lapply(data$documents[], length)
weights <- c()
i <- 1
while (i <= length(doc_lengths)) {
  weights[[i]] <- doc_lengths[[i]]/2
  i <- i + 1}
mean_weight <- mean(weights)
props_unweighted <- colMeans(mod_prev$theta[,1:K])
props_weighted <- colMeans((mod_prev$theta*weights/mean_weight)[, 1:K])

topic_labels_unlisted <- unlist(topic_labels, use.names = FALSE)

props_df <- data.frame(topic_labels_unlisted, props_unweighted, props_weighted) %>% reshape2::melt(id = "topic_labels_unlisted")
colnames(props_df) <- c("topic", "variable", "proportion")
```

Next, we identify two ways to calculate global topic proportions: either as the simple (unweighted) average of $\theta_d$ across all documents (i.e., as the average of MP-level proportions across all MPs); or by weighting each $\theta_d$ by the number of words in the respective documents, $N_d$. The table below shows all topics with their respective global proportions, for both weighting methodologies. We observe that for most topics, weighted and unweighted proportions are rather similar, but there are exceptions. In particular, the topics concerned with everyday political tweets have much higher unweighted than weighted frequencies; this makes sense, however, since such "diplomatic" tweets tend to be shorter than those which actually discuss a specific content.

```{r echo=FALSE}
ggplot(data = props_df, aes(x = topic, y = proportion, fill = variable)) + 
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("grey40","grey80")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1),
        axis.title.x = element_text(size = 12, face = "bold"),
        axis.title.y = element_text(size = 12, face = "bold"))
```

While labelling tells us which words best represent each topic - and thus, what each topic truly represents - it does not yet tell us to which extent individual topics are related to each other. In the graph below, we visualize the similarity of two topics, Topic 3 (`r topic_labels[3]`) and Topic 6 (`r topic_labels[6]`), in terms of their vocabulary usage. As suggested by the topic labels already, there is a significant overlap in vocabulary usage.

```{r echo=FALSE}
# vocabulary usage comparison for two topics
plot(mod_prev, type = "perspectives", topics = c(3, 6), n = 30)
```

More generally, we can evaluate the connectedness between different topics with the topic correlation matrix of the correlations between document-level topic proportions $\theta_d$. This is visualized in the graph below.

```{r echo=FALSE}
# global topic correlation
cormat <- cor(mod_prev$theta)
ggcorrplot::ggcorrplot(cormat) +
  scale_x_continuous(breaks = seq(1, 15, by = 1)) +
  scale_y_continuous(breaks = seq(1, 15, by = 1)) +
  labs(x = "topic number", y = "topic number") +
  theme_minimal() +
    theme(axis.title.x = element_text(size = 12, face = "bold"),
          axis.title.y = element_text(size = 12, face = "bold"))
```

Most topics are negatively correlated with each other, which does not come as a surprise, given the relatively low total number of topics, `r K`, and that topic proportions are "supplements": the higher one topic proportion, the lower the total of the others. Moreover, most topic correlations are rather weak in absolute size: the strongest negative  correlation (`r scales::percent(cormat[1,15], accuracy = 0.01)`) is the one between  topic  1 (`r topic_labels[[1]]`) and topic 15 (`r topic_labels[[15]]`), while the strongest positive correlation (`r scales::percent(cormat[10,13], accuracy = 0.01)`) is the one shown before, between (`r topic_labels[10]`) and (`r topic_labels[13]`).

We can also visualize these correlations using a network graph, where topics are connected whenever they are positively correlated. We observe three small clusters as well as some isolated topics, one of them being topic 8, `r topic_labels[8]`, which makes sense since it only entered the public sphere in early 2020, i.e., during the last months of our data collection period. In general, the relationships between the topics, as depicted below, are in line with their labelling.

```{r echo=FALSE}
set.seed(111)
mod_prev_corr <- topicCorr(mod_prev, method = "simple", cutoff = 0.00,
                           verbose = TRUE) # based on correlations between mod_prev$theta
vertex_sizes <- rep(20, K)
plot.topicCorr(mod_prev_corr, vlabels = topic_labels, vertex.label.cex = 1, vertex.size = vertex_sizes)
```



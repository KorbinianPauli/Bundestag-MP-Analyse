---
title: "4_5"
author: "Patrick Schulze, Simon Wiegrebe"
date: "June 2020"
output:
  pdf_document:
    toc: true
    number_sections: true
  html_document: default
bibliography: bibliography.bib
biblio-style: myabbrvnat
link-citations: yes
linkcolor: blue
header-includes:
  - \usepackage{caption}
  - \usepackage{float}
  - \captionsetup{labelformat=empty}
  - \usepackage{multirow}
  - \usepackage{graphicx}
  - \usepackage{booktabs}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Preparation, include=FALSE}
# ----------------------------------------------------------------------------------------------
# ---------------------------------------- Preparation -----------------------------------------
# ----------------------------------------------------------------------------------------------

# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
  "betareg", "ggcorrplot", "grid", "gridExtra", "huge", "knitr", "mvtnorm", 
  "quanteda", "reshape2", "scales", "stm", "stmprevalence", "stringi", "tidyverse", "tm"
)
not_installed <- packages_required[!packages_required %in%
                                     installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
  lapply(
    not_installed,
    install.packages,
    repos = "http://cran.us.r-project.org",
    dependencies = TRUE,
    type = itype
  )
}
lapply(packages_required, library, character.only = TRUE)

# set working directory
# setwd('C:\\Users\\Simon\\OneDrive\\Uni\\LMU\\SS 2020\\Statistisches Consulting\\Bundestag-MP-Analyse')
# setwd("/Users/patrickschulze/Desktop/Consulting/Bundestag-MP-Analyse")
```

# Results

## Hyperparameter Search and Model Fitting

## Labeling

## Global-level Topic Analysis

## Covariate-level Topic Analysis

## Content Model

## 2-Step Procedure: CTM

```{r include=FALSE}
# load data
data <- readRDS("../data/preprocessed_monthly.rds")
colnames_table <- read.csv(file = "../data/topic_monthly_colnames.csv")

# data_corpus <- readRDS("../data/prep_monthly.rds")
# data_aggregated <- readRDS("../data/preprocessed.rds") # MP-level (non-monthly) data
```

```{r echo=FALSE}
K <- 15
```

```{r echo=FALSE}
# choose covariates and number of topics
covar <- "Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) +
  s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)"
outcome <- ""
prevalence <- as.formula(paste(outcome, covar, sep = "~"))

# # fit model on training data
# mod_ctm <- stm::stm(
#   documents = data$documents,
#   vocab = data$vocab,
#   data = data$meta,
#   K = K,
#   # prevalence = prevalence,
#   # gamma.prior = 'L1',
#   seed = 123,
#   max.em.its = 300,
#   init.type = "Spectral")
# saveRDS(mod_ctm, "../data/mod_ctm_monthly.rds")

mod_ctm <- readRDS("../data/mod_ctm_monthly.rds")

mod_prev <- readRDS("../data/mod_prev_monthly.rds")

# load topic labels
topic_labels <- list(
  Topic1 = "Right/Nationalist",
  Topic2 = "Miscellaneous 1",
  Topic3 = "Climate Economics",
  Topic4 = "Social/Housing",
  Topic5 = "Digital/Future",
  Topic6 = "Climate Protection",
  Topic7 = "Europe",
  Topic8 = "Corona",
  Topic9 = "Left/Anti-war",
  Topic10 = "Twitter/Politics 1",
  Topic11 = "Twitter/Politics 2",
  Topic12 = "Miscellaneous 2",
  Topic13 = "Twitter/Politics 3",
  Topic14 = "Right-wing Extremism",
  Topic15 = "Society/Solidarity"
)

# load list of prevalence covariates
varlist <- c(
  "t", "Partei", "Bundesland", "Struktur_4", "Struktur_22", "Struktur_42", "Struktur_54"
)
# load full names of prevalence covariates
varlist_fullnames <- c(
  "Time", "Party", "Federal State", "Immigrants (%)", "GDP per capita", 
  "Unemployement Rate (%)", "vote share (%)"
)

formula <- 1:15~Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) + 
 s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)
```

## 2-Step Approach: CTM

As briefly mentioned in section 2 already, a point of concern when using the STM is the double usage of (prevalence) covariates: they are used in the estimation of the topic itself (and thus, in the estimation of the latent topic proportions) and subsequently they are again used in metadata inference.

[Insert some of the initial part of section 4.7 on train-test-split]

To avoid overfitting due to double usage of covariates, we fit an STM without including any covariates in the model estimation, thus reducing the model to a simple CTM. In a second, isolated step, we estimate the relationship between topic proportions and covariates. That is, we forgo the potential (small) gains of joint estimation of the STM in favor of a clear-cut two-step procedure which avoids overfitting.

```{r include=FALSE}
## topic proportions by party
heldout <- make.heldout(
                        documents = data$documents,
                        vocab = data$vocab,
                        N = floor(0.1 * length(data$documents)),
                        proportion = 0.5,
                        seed = 123)

# mod_prev_heldout <- stm::stm(
#                       documents = heldout$documents,
#                       vocab = heldout$vocab,
#                       data = data$meta,
#                       K = K,
#                       prevalence = prevalence,
#                       gamma.prior = 'L1',
#                       seed = 123,
#                       max.em.its = 300,
#                       init.type = "Spectral")
# saveRDS(mod_prev_heldout, "../data/mod_prev_heldout_monthly.rds")
mod_prev_heldout <- readRDS("../data/mod_prev_heldout_monthly.rds")

# mod_ctm_heldout <- stm::stm(
#                       documents = heldout$documents,
#                       vocab = heldout$vocab,
#                       data = data$meta,
#                       K = K,
#                       # prevalence = prevalence,
#                       # gamma.prior = 'L1',
#                       seed = 123,
#                       max.em.its = 300,
#                       init.type = "Spectral")
# saveRDS(mod_ctm_heldout, "../data/mod_ctm_heldout_monthly.rds")
mod_ctm_heldout <- readRDS("../data/mod_ctm_heldout_monthly.rds")

eval.heldout(mod_prev_heldout, heldout$missing)[["expected.heldout"]]
eval.heldout(mod_ctm_heldout, heldout$missing)[["expected.heldout"]]

```

```{r echo=FALSE}
diff_matrix <- mod_prev$theta - mod_ctm$theta
abs_diff_matrix <- abs(diff_matrix)
```

As a first step, we fit the CTM analogously to the original STM (which includes topical prevalence variables), the only difference being that no document-level metadata is used in the estimation of the CTM. In line with the performance results in @roberts2016model, we observe a slightly higher held-out likelihood for the STM (-8.5478) than for the CTM (-8.5492) when holding out a random 50% of the words from a randomly chosen 10% of the documents. Moreover, we notice that the topics themselves (in terms of their top words) are almost identical to those of the STM, which is why we use the same topic labeling as in section 4.4. As for differences in topic proportions between the two models on a document level, we consider the average topic proportion deviation per document, $\frac{1}{K}\sum_{k=1}^{K}|\theta_{d,k}(STM)-\theta_{d,k}(CTM)|$. The resulting average difference between topic proportions per topic, averaged across all documents, amounts to `r scales::percent(mean(rowMeans(abs_diff_matrix)), accuracy = 0.01)`; that is, for an average document, the absolute difference in the proportion of each topic is less than `r scales::percent(mean(rowMeans(abs_diff_matrix)))`, which is rather moderate. These differences in topic proportions between STM and CTM further cancel each other out across documents: when comparing *global* topic proportions (i.e., topic proportions simply averaged across all documents), the results are very similar, with the average difference per topic only amounting to  `r scales::percent(mean(abs(colMeans(diff_matrix))), accuracy = 0.01)`. Altogether, topic proportions seem to be affected by the topical prevalence covariates to a small degree on an individual document level, and this effect almost disappeares entirely if we consider corpus-wide topic proportions.

[Update the below according to section 4.4 updates in graphs]

In the second step, we consider the relationship between topic proportions and covariates for the CTM and compare the resulting relationships with those of the originally fitted STM (which contains prevalence covariates). For comparability, we use the same methodology as in section 4.4: applying the method of composition with a quasibinomial regression of individual topic proportions on covariates. The only difference is that prevalence covariates were not included in the model used to generate topic proportions. Consequently, sampling all (unnormalized) topic proportions jointly via the logistic normal distribution (as in Figure 4.XXX) is not applicable here, as no $\Gamma$-vector is being estimated at all. In the figure below, we visualize the CTM topic proportion of three topic across parties. Comparing the results to the corresponding ones of the STM (Figure 4.XXX), we see a similar pattern for all but the green party and a general shift in scale across all parties when considering the "XXX" topic. For the "XXX" topic the average proportions by party show a higher degree of resemblance across models. Finally, for the right/national topic, there is a substantial difference between the two models in terms of the topic proportion of the AfD party. We observe the same similarities and differences between the two models if we use beta regression instead of quasibinomial regression within the method of composition, corroborating our results (see appendix).

[Graph quasi_t134_cont_ctm from ../plots/4_5]


For continuous covariates (see appendix), the comparison of STM and CTM yields very similar results to those of the per-party comparison: for the "XXX" topic, the trends are not that similar anymore and the scale differs (as above, with higher overall topic proportions according to the CTM). For the "XXX" topic, on the other hand, the relationship between the covariates and topic proportion does not differ very much, neither in trend nor in scale. 

All in all, the comparison of 



To be addressed:
* metadata for test data is entirely meaningless, does not affect topic proportions at all (given words!!!)
* manipulating covariate values neither
* train/test: once words are given, covariates do not have any further impact: change party for MD (or exclude newData), show: no effect on predicted topic proportions (-> for future research: predict topic proportions based on document covariates only)
* train/test: change formulation, since covariates do not really "generate" topic proportions; don't mention causal inference
* train/test: main point of section: validate model: do topics (and their proportions) make sense?
* additional paragraph for two-step procedure (+ 1 top graph)

---
title: "1_introduction"
author: "Patrick Schulze, Simon Wiegrebe"
date: "June 2020"
output:
  pdf_document:
    toc: true
    number_sections: true
  html_document: default
bibliography: bibliography.bib
biblio-style: myabbrvnat
link-citations: yes
linkcolor: blue
header-includes:
  - \usepackage{caption}
  - \usepackage{float}
  - \captionsetup{labelformat=empty}
  - \usepackage{multirow}
  - \usepackage{graphicx}
  - \usepackage{booktabs}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Preparation, include=FALSE}
# ----------------------------------------------------------------------------------------------
# ---------------------------------------- Preparation -----------------------------------------
# ----------------------------------------------------------------------------------------------

# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
  "betareg", "ggcorrplot", "grid", "gridExtra", "huge", "knitr", "mvtnorm", 
  "quanteda", "reshape2", "scales", "stm", "stmprevalence", "stringi", "tidyverse", "tm"
)
not_installed <- packages_required[!packages_required %in%
                                     installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
  lapply(
    not_installed,
    install.packages,
    repos = "http://cran.us.r-project.org",
    dependencies = TRUE,
    type = itype
  )
}
lapply(packages_required, library, character.only = TRUE)

# set working directory
# setwd('C:\\Users\\Simon\\OneDrive\\Uni\\LMU\\SS 2020\\Statistisches Consulting\\Bundestag-MP-Analyse')
# setwd("/Users/patrickschulze/Desktop/Consulting/Bundestag-MP-Analyse")
```

# Introduction

The rise in popularity of social media has changed various aspects of private, public, and professional life over the last two decades. (Quote) From an academic point of view, this has led to an unprecedented increase in the supply of publicly available unstructured text data, ready to be analyzed. (Quote) At the same time, advances in the field of machine learning, particularly in *Natural Language Processing* (NLP), have opened numerous new opportunities for the analysis of such large-scale unstructured texts.

A field which has been particularly impacted by the usage of social media (and the information extracted from it) is politics. At least since the 2016 Brexit vote and US presidential election, politicians have come to recognize not only that social media presence is ever more important, but also how strong a message their social media behavior can transmit. (Quote) Among social media networks, Twitter is of particular importance, since it allows for direct communication between politicians and voters and even more so after the Facebook-Cambridge Analytica data breach in 2018.

As a consequence, there has been increasing academic interest in text-based (intra- and inter-)party politics [e.g., @ceron2017intra; @daniel2019static; @grimmer2010bayesian; @quinlan2018show]. This way, unstructured text and the insights generated from it can subsequently be used as input for a broad variety of tasks, ranging from election forecasts [e.g., @burnap2016140; @jungherr2016twitter; @tumasjan2010predicting] to prediction of stock market movements [@nisar2018twitter].

The key challenge in analyzing large amounts of unstructured text is to reduce dimensionality and classify the pieces of text: either into previously determined categories (for instance, sentiments), which corresponds to a supervised learning problem; or by trying to discover latent thematic clusters that govern the content of the documents, which is now an instance of unsupervised learning (since the number and labeling of clusters is to be determined). In this paper, we pursue the second strategy, usually referred to as *topic modeling*, and apply it to German politics. In particular, we construct a dataset where the text documents consist of Twitter messages by German Members of Parliament (MPs) and which furthermore contains a plenitude of personal MP-level data as well as socioeconomic data on an electoral-district level. Subsequently, we fit a topic model to the data to discover latent topics and analyze their relationship with document-level metadata. Due to the difficulties regarding causal inference within (latent variable-based) topic models, the analysis presented in this paper is mostly explorative/descriptive with a focus on statistical and methodological soundness instead of specific (politological) hypothesis testing.

We find that... (tendencies, what is possible, what not)

The remainder of this paper is organized as follows. Section 2 provides the theoretical foundation of topic modeling, in particular the "component models" of the *Structural Topic Model* which we use for the major part of our analysis, as well as a brief discussion on inference and parameter estimation. Section 3 describes the data collection process, the data itself, and the data preprocessing necessary for topic modeling. Section 4 presents the results, including a discussion of inference strategies and an alternative modeling procedure. Finally, section 5 concludes.


---
title: 'Draft June 2020'
author: "Patrick Schulze, Simon Wiegrebe"
date: "June 2020"
output:
  pdf_document:
    toc: true
    number_sections: true
  html_document: default
bibliography: bibliography.bib
biblio-style: myabbrvnat
link-citations: yes
linkcolor: blue
header-includes:
  - \usepackage{caption}
  - \usepackage{float}
  - \captionsetup{labelformat=empty}
  - \usepackage{multirow}
  - \usepackage{graphicx}
  - \usepackage{booktabs}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Preparation, include=FALSE}
# ----------------------------------------------------------------------------------------------
# ---------------------------------------- Preparation -----------------------------------------
# ----------------------------------------------------------------------------------------------

# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
  "betareg", "ggcorrplot", "grid", "gridExtra", "huge", "knitr", "mvtnorm", 
  "quanteda", "reshape2", "scales", "stm", "stringi", "tidyverse", "tm"
)
not_installed <- packages_required[!packages_required %in%
                                     installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
  lapply(
    not_installed,
    install.packages,
    repos = "http://cran.us.r-project.org",
    dependencies = TRUE,
    type = itype
  )
}
lapply(packages_required, library, character.only = TRUE)

# set working directory
# setwd('C:\\Users\\Simon\\OneDrive\\Uni\\LMU\\SS 2020\\Statistisches Consulting\\Bundestag-MP-Analyse')
# setwd("/Users/patrickschulze/Desktop/Consulting/Bundestag-MP-Analyse")
```

# Introduction

TBD

# Theoretical Framework

## STM - Introduction

The Structural Topic Model (STM) is a topic model which extends classical topic models such as Latent Dirichlet Allocation (LDA) by incorporating information of covariates. Topic models can be used to infer topics from a large text corpus grouped into documents. In topic modelling it is assumed that this corpus is generated from a small number of distributions over words, the topics. The proportions of these topics are document-specific. In contrast to simpler topic models such as LDA, the STM relates topic proportions to document-level covariates. Furthermore, each distribution over words, i.e. each topic, can vary for different documents dependent on the covariate values of this document.

The detailed mechanism underlying the STM can be illustrated using its graphical model representation (see Figure 1). As outlined above, for each document indexed by $d \in \{1,\dots,D\}$ there exists a $K-1$-dimensional vector $\theta_d$ of topic proportions. Topic proportions are assumed to depend on $P$ document-specific so-called topical prevalence covariates $X \in \mathbb{R}^{D \times P}$, by following a logistic normal distribution with mean $X_d\Gamma$, where $\Gamma = [\gamma_1|\dots|\gamma_k]$, and covariance $\Sigma$. Each of the $N_d$ words in document $d$ is subsequently assigned to one of the $K$ topics dependent on the topic proportions $\theta_d$; this per-word topic assignment is captured by the latent variable $z_{d,n} \sim \text{Multinomial}_K(\theta_d)$, where $n \in \{1,\dots,N_d\}$ denotes the word index. As stated, the distribution over words that characterizes a topic can vary for each document dependent on document-specific covariates $Y \in \mathbb{R}^{D \times A}$, the so-called topical content covariates. A word $w_{d,n}$ is then the result of the assigned topic, expressed by $z_{d,n}$, the content covariates $Y_d$, and their interactions. More precisely, this last step is intuitively best understood as a multinomial logistic regression of the words on the latter variables. A word $w_{d,n}$ then ultimately follows a multinomial distribution with probabilities $\beta_{d,n} := \beta(z_{d,n}, Y_d) \in \mathbb{R}^V$, i.e.\ $w_{d,n} \sim \text{Multinomial}_V(\beta_{d,n})$, where $V$ denotes the total number of distinct words in the corpus; for details on the exact specification of $\beta_{d,n}$ see p.\ 991, @roberts2016. Thus the occurence of a word (which is equivalent to being drawn from a corresponding multinomial distribution) depends on the topic assignment as well as on the topical content covariates, where the topic assignment itself is a function of the topical prevalence covariates.

![Graphical model representation of the STM](../data/stm_graphical.png)

## STM - Scope

Topic models are unsupervised learning methods, since the true topics from which the text was generated are not known. Thus, traditionally topic models have been used as an exploratory tool providing a concise summary of topics, where it is hoped that the posterior induces a good decomposition of the corpus. Topic models have also been used for tasks such as collaborative filtering and classification (see e.g.\ @blei2003latent). In particular, they can be used as a dimensionality reducing method in semi-supervised learning methods. Such a process can in general be described as a two-stage approach, where in the first stage topic proportions and content are learned, and in the second stage a supervised method such as regression takes this learned representation as input. 

The fundamental idea of STMs is to combine these two steps: Topics and their relation to covariates are jointly estimated. For instance, the estimated effect of topical prevalence covariates $X_d$ on topic proportions is reflected in the estimate of $\Gamma$. However, since the topic proportions $\theta_d$ are random variables, it is a better approach to incorporate the uncertainty of $\theta_d$, accesible through the estimated approximation of the posterior $p(\theta_d | \Gamma, \Sigma, X)$, when determinig the effect of covariates on topic proportions. This is achieved by what is called the "method of composition" in social sciences: By sampling from the approximate posterior for $\theta$ and subsequently regressing these topic proportions on $X$ it is possible to integrate out the topic proportions (since these are latent variables!) and obtain an i.i.d.\ sample from the marginal posterior of the regression coefficients for the topical prevalence covariates.

A problem we see with this approach is, however, that the same covariates and in general the same data used to infer the topical structure are subsequently used to determine effetcs of the former on the latter (or vice versa). This problem has recently also been adressed by @egami2018make. In practice, in case of the regression coefficients for the topical prevalence covariates (obtained using the method of composition as outlined above), due to the regularizing priors for $\Gamma$ we have found that the prevalence covariates have almost no influence on the estimated topic proportions. Thus the regression coefficients (with the topic proportions as the dependent variable) should not be largely affected by this problem. However, the question then appears why the covariate variables have been used to obtain the topical structure in the first place. In an empirical evaluation @roberts2016 showed that the STM consistently outperformed other topic models such as LDA, when comparing the respective heldout likelihoods in different settings. This indicates that the STM performs better at predicting the topical structure by incorporating covariates, regardless of the concrete specification of these covariates.

Nevertheless, it should in each case be investigated whether the relationship of variables implied by the STM is valid. For instance, we have split our data into training and test sets and found that the topical structure predicted on the test set differs starkly from the structure on the training set. This could of course be caused by a misspecification of the topical prevalence and content variables. However, since the topical prevalence covariates have almost no influence on the estimated topic proportions on the training set due to the regularizing priors (and e.g.\ likewise on the heldout likelihood that can be used for validation), it is practically impossible to validate a good prevalence specification.

## Posterior Distribution

The posterior given on p.\ 992, @roberts2016, can be derived as follows:

\begin{align*}
p(\eta, z, \kappa, \Gamma, \Sigma | w, X, Y) & \propto \underbrace{p(w | \eta, z, \kappa, \Gamma, \Sigma, X, Y)}_{=p(w | z, \kappa, Y)} p(\eta, z, \kappa, \Gamma, \Sigma | X, Y) \\
& \propto p(w | z, \kappa, Y) p(z | \eta) p(\eta | \Gamma, \Sigma, X) \prod p(\kappa) \prod p(\Gamma)p(\Sigma) \\
& \propto \Big\{ \prod_{d=1}^{D} p(\eta_d | \Gamma, \Sigma, X_d) \Big( \prod_{n=1}^{N} p(w_n | \beta_{d, n}) p(z_{d,n} | \theta_d) \Big) \Big\} \prod p(\kappa) \prod p(\Gamma) p(\Sigma) \\
& \propto \Big\{ \prod_{d=1}^{D} \text{Normal}(\eta_d | X_d \Gamma, \Sigma) \Big( \prod_{n=1}^{N} \text{Multinomial}(z_{n,d}| \theta_d) \\
& \ \ \ \ \times \text{Multinomial}(w_n | \beta_{d,n}) \Big) \Big\} \times \prod p(\kappa) \prod p(\Gamma) p(\Sigma),
\end{align*}

where $\beta_{d, n}:= \beta(z_{d,n}, Y_d) \in \mathbb{R}^V$ with entries $\beta_{d, k, \nu} \propto \exp(m_{\nu} + \kappa_{k,\nu}^{(t)} + \kappa_{y_d,\nu}^{(c)} + \kappa_{y_d, k,\nu}^{(i)})$, $\nu \in \{1,\dots,V\}$, and $\theta_d := \text{softmax}(\eta_d)$.

# Data

```{r echo=FALSE, include=FALSE}
# load twitter data and aggregate on a per-user basis
topic <- readRDS("../data/topic.rds")
prep <- readRDS("../data/prep.rds")

topic_user <- topic %>% 
  group_by(Name) %>% 
  mutate(Tweets_Dokument = paste(Tweets, collapse = ' ')) %>%
  summarize(
    Twitter_Username = max(Twitter_Username), 
    Tweets_Dokument = max(Tweets_Dokument),
    Anzahl_Follower = max(Anzahl_Follower)
  ) %>%
  ungroup()

# merge with personal and socioeconomic data
abg_df <- read_delim("../data/abg_df.csv", delim = ",") %>%
  rename(Twitter_Username = Twitter, Wahlkreis_Nr = `Wahlkreis-Nr.`)
se_df <- read_delim("../data/se_df.csv", delim = ",") %>% 
  rename(Wahlkreis_Nr = `Wahlkreis-Nr.`, "AfD Anteil" = "AFD Anteil") %>% 
  select(-Bundesland)

alldata <- topic %>% 
  inner_join(abg_df) %>% 
  inner_join(se_df, by = "Wahlkreis_Nr")

alldata <- alldata %>% 
  filter(Partei != "fraktionslos")

```

## Data Collection

As a first step towards applying the STM to German political entities, we constructed a database with personal information about all German MPs. Using Python's BeautifulSoup web scraping tool as well as a selenium webdriver, we gathered data such as name, party, and electoral district from the [official parliament website](https://www.bundestag.de/abgeordnete) for all of the 709 members of the German parliament during its 19th election period, elected on September 24, 2017. (Footnote: MPs who resigned or passed away since this date were also listed on the website and thus included initially; they were manually excluded from further analysis.)

Since information on social media profiles was scarce and incomplete on the official parliament website, we scraped official party homepages for each of the six political parties represented in the current parliament. MPs who did not provide a Twitter account either on the official parliament website or on their party's official homepage were excluded. Using Python's tweepy library to access the official Twitter API, we scraped all tweets by German MPs from September 24, 2017 through April 24, 2020, i.e., during a total of 31 months. (Footnote: tweepy restricts the total number of retrievable tweets to 3,200. For those MPs with a larger number of tweets, the most recent 3,200 tweets are taken into account. However, this only affects two MPs.) This initially yielded `r nrow(topic)` tweets from a total of `r nrow(topic_user)` members of parliament.

To complement personal data, we also gathered socioeconomic data such as GDP per capita and unemployment rate as well as 2017 election results on an electoral-district level for all of the 299 electoral districts, from the [official electoral website](https://www.bundeswahlleiter.de). After removing  independent MPs as well as MPs without a specific electoral district assigned to them (for matchability with socioeconomic data), the final dataset counted `r nrow(prep)` MPs. The corresponding total number of tweets amounted to `r nrow(alldata)`. The table below shows total monthly tweet frequencies for our period of analysis, September 24, 2017 through April 24, 2020. As can be seen, tweet frequencies - though fluctuating - increase over time, peaking at almost 20,000 in March 2020.

```{r echo=FALSE, fig.align='center', fig.asp=.6, fig.width=10, warning=FALSE, out.width='82%'}

data_adj <- alldata %>% mutate(Jahr = lubridate::year(Datum), Monat = lubridate::month(Datum))
data_adj$date <- with(data_adj, sprintf("%d-%02d", Jahr, Monat))
ggplot(data_adj, aes(x = date)) + geom_histogram(stat = "count") +
  labs(x = "time", y = "# tweets")  +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1),
        axis.title.x = element_text(size = 12, face = "bold"),
        axis.title.y = element_text(size = 12, face = "bold"))
```

Next, data were grouped and tweets concatenated on a per-user level (thus aggregating tweets across the entire 31 months) as well as on a per-user per-month level, yielding a user-level and a (user-level) monthly dataset. This means that a document represents the concatenation of *all* of a single MP's tweets for the user-level dataset and a single MP's *monthly* tweets for the monthly dataset. This also means that MP-level metadata such as personal information and socioeconomic data (through the electoral district matching) can be used as document-level covariates. For the monthly dataset, the temporal component (year and month) constitutes an additional covariate. At this point, the data preparation was completed, thus marking the starting point of the preprocessing required for topic analysis, which is identical for both datasets.

## Data Preprocessing

We used the *quanteda* package within the R programming language for preprocessing. As a first step, we built a quanteda corpus from all documents, already transcribing German umlauts *ä/Ä*, *ö/Ö*, *ü/Ü* as well as German ligature *ß* as *ae/Ae*, *oe/Oe*, *ue/Ue*, and *ss* and removed hyphens. Next, we transformed the text data into a quanteda document-feature matrix (DFM), which essentially tokenizes texts, thereby convering all characters to lowercase. From the DFM, we removed an extensive list of German stopwords, using the [stopwords-iso GitHub repository](https://github.com/stopwords-iso/stopwords-iso), as well as English stopwords included in the *quanteda* package. Moreover, hashtags, usernames, quantities and units (e.g., *10kg* or *14.15uhr*), interjections (e.g., *aaahhh* or *ufff*), terms containing non-alphanumerical characters, meaningless word stumps (e.g., *innen* from the German female plural declension or *amp*, the remainder left after removing the ampersand sign, *&*) were removed. Terms with less than four characters and terms with a term frequency (overall number of occurrences) below five or with a document frequency (number of documents containing the word) below three were excluded. Finally, we manually removed over-frequent terms that would diminish the distinguishability of topics, such as *bundestag* or *polit*. 

We also performed word-stemming, which means cutting off word endings to remove discrepancies arising purely from declensions or conjugations - of particular importance for the German language. Due to the nature of the German language, the additional gains of lemmatization (which aims at identifying the base form of each word) would only be small as compared to the large increase in complexity, which is why we decided to use stemming only. Another issue when dealing with German language documents are compound words, which are sometimes hyphenated, basically leading to a distinction where semantically there is none. We addressed this issue by removing hyphens in the very beginning of the preprocessing and converting all terms to lowercase, thus "gluing together" compound words; this way, terms like *Bundesregierung* and *Bundes-Regierung* are both transformed into *bundesregierung* (and, after stemming, into *bundesregier*). Finally, automatic segmentation techniques were not necesssary for the German language (@lucas2015computer).

```{r include=FALSE}
# load data
data <- readRDS("../data/preprocessed_monthly.rds")
colnames_table <- read.csv(file = "../data/topic_monthly_colnames.csv")
data_corpus <- readRDS("../data/prep_monthly.rds")

data_aggregated <- readRDS("../data/preprocessed.rds") # MP-level (non-monthly) data
```

As the result of preprocessing, one empty MP-level document was dropped, so that a total of `r nrow(data$meta)` MP-level documents were eventually analyzed, each one associated with `r ncol(data$meta)` covariates.

# Results

## Hyperparameter Search and Model Fitting

```{r include=FALSE}
# # search hyperparameter space for optimal K
# hyperparameter_search <- searchK(
#   documents = data$documents,
#   vocab = data$vocab,
#   data = data$meta,
#   # K = c(5,6,7,8,9,10),
#   K = c(5,10,15,20,25,30,35,40),
#   prevalence =~ Partei,
#   max.em.its = 150,
#   init.type = "Spectral"
# )

# load searchK results
searchK_data <- readRDS("../data/searchK_large_data.rds")
```

Throughout the topic analysis, we use the *stm* package, which is implemented in the R programming language (@stm). The most important choice when fitting an STM is the number of topics, K. While there is no *true* or *optimal* number of topics, we explore the hyperparameter space using the *searchK* function to get an understanding of the impact of K on model fit. We use four of the metrics that come with this function, *held-out likelihood*, *semantic coherence*, *exclusivity*, and *residuals*. As for the first one, the *searchK* function randomly holds out a proportion of some of the documents. This set of held-out words is then used to evaluate their probability given the trained model, giving rise to the *held-out likelihood*. Regarding the second metric, a model with K topics is *semantically coherent* whenever those words that characterize a specific topic (i.e., the most frequent words within a topic) also do appear in the same documents. *Exclusivity* basically tells us to which degree a topic's word *only* occur in that topic (for more detail, see the *FREX* methodology further below). Finally, *residuals* is a metric based on residual dispersion, which theoretically should be equal to one; so if the observed residuals exceed this value, the number of topics was most likely chosen *insufficiently small*.

Another aspect to be taken into account when choosing K (or, to be precise, when choosing a search grid for searchK) is interpretability. While a large K certainly allows for a more fine-grained determination of topics, the resulting topics might be rather difficult to label. Furthermore, for large K we would obtain many topics which could be considered sub-topics of the topics we would obtain when using a smaller value for K. The graph below shows the four metrics, as introduced above, for values of K between 5 and 40 (in steps of 5).

```{r echo=FALSE}
plot_heldout <- ggplot(data = searchK_data$results, aes(x = K, y = heldout)) +
                  geom_line() +
                  geom_point() +
                  labs(y = "held-out likelihood") +
                  theme_minimal() +
                  theme(axis.text.x = element_text(vjust = 1, size = 12, hjust = 1),
                        axis.title.x = element_text(size = 12, face = "bold"),
                        axis.title.y = element_text(size = 12, face = "bold"))

plot_semcoh <- ggplot(data = searchK_data$results, aes(x = K, y = semcoh)) + 
                  geom_line() +
                  geom_point() +
                  labs(y = "semantic coherence") +
                  theme_minimal() +
                  theme(axis.text.x = element_text(vjust = 1, size = 12, hjust = 1),
                        axis.title.x = element_text(size = 12, face = "bold"),
                        axis.title.y = element_text(size = 12, face = "bold"))

plot_exclus <- ggplot(data = searchK_data$results, aes(x = K, y = exclus)) +
                  geom_line() +
                  geom_point() +
                  labs(y = "exclusivity") +
                  theme_minimal() +
                  theme(axis.text.x = element_text(vjust = 1, size = 12, hjust = 1),
                        axis.title.x = element_text(size = 12, face = "bold"),
                        axis.title.y = element_text(size = 12, face = "bold"))

plot_residual <- ggplot(data = searchK_data$results, aes(x = K, y = residual)) + 
                  geom_line() +
                  geom_point() +
                  labs(y = "residuals") +
                  theme_minimal() +
                  theme(axis.text.x = element_text(vjust = 1, size = 12, hjust = 1),
                        axis.title.x = element_text(size = 12, face = "bold"),
                        axis.title.y = element_text(size = 12, face = "bold"))

gridExtra::grid.arrange(plot_heldout, plot_semcoh, plot_exclus, plot_residual, ncol=2)

K <- 15
```

Both 15 and 20 topics seem to be good trade-offs between the metrics used. Taking into account the interpretability aspect, we opt for K = `r K`.

Before fitting the model, we need to choose the document-level covariates we want to include. Since a topic model is explorative by definition, we simply include those covariates that seem to be most influential *a priori*: party and state (both categorical), date (as smooth effect), as well as percentage of immigrants, GDP per capita, unemployment rate, and the 2017 election results of the MP's respective party (the last four as smooth effects and on an electoral-district level).

```{r echo=FALSE}
# choose covariates and number of topics
prev_var <- c("Partei", "Bundesland", "Datum", "Struktur_4", "Struktur_22", "Struktur_42", "Struktur_54")
outcome <- ""
prevalence <- as.formula(paste(outcome, paste(prev_var, collapse = "+"), sep = "~")) 

# # fit model
# mod_prev <- stm::stm(
#   documents = data$documents,
#   vocab = data$vocab,
#   data = data$meta,
#   K = K,
#   prevalence = prevalence,
#   # gamma.prior = 'L1',
#   seed = 123,
#   max.em.its = 200,
#   init.type = "Spectral")

mod_prev <- readRDS("../data/mod_prev_monthly.rds")
```

## Labelling

As a starting point subsequent to model fitting, we visually inspect the resulting topics, in particular their best words, as demonstrated in the output below. There are different metrics for evaluating which words are most representative of a topic. The STM comes with four such metrics: *highest probability*, *FREX*, *Lift*, and *Score*. The first one, *highest probability*, simply outputs for each topic those words in the topic-specific word vector, $\beta_{d,n}$, with the highest corpus frequency, i.e, those with the highest absolute frequency across all documents. *FREX* takes into account not only how frequent but also how exclusive words are: for a given topic, it is calculated by taking the harmonic mean of i) the word's rank by probability within the topic (frequency) and ii) the topic's rank by  word frequency  across all topics (exclusivity). Further information on the estimation of *FREX*, *Lift*, and *Score* can be found in @bischof2012summarizing, in the R package *lda* (@chang2010package), and in @taddy2012estimation, respectively.

```{r echo=FALSE}
# labeling workflow (for each topic): 

## (1) inspect most frequent words per topic (using different metrics as well as word cloud)
## (2) evaluate most representative documents per topic
## (3) assign label

# first, prepare objects/variables needed for labelling process

## table of MAP topic proportions per document (for all topics)
topic_props <- make.dt(
  mod_prev, 
  data$meta[c("Name", "Partei", "Datum", "Bundesland")]) %>% 
  cbind(docname = names(data$documents), .)

## top words per topic (for all topics)
n <- 5
topic_words <- labelTopics(mod_prev, n = n)

## topic to be evaluated
topic_number <- 1
topic_number_long <- paste0("Topic", topic_number)

## number of top documents to be printed in step (2)
docs_number <- 10

## initialize list with empty labels
topic_labels <- list(
  Topic1 = NULL,
  Topic2 = NULL,
  Topic3 = NULL,
  Topic4 = NULL,
  Topic5 = NULL,
  Topic6 = NULL,
  Topic7 = NULL,
  Topic8 = NULL,
  Topic9 = NULL,
  Topic10 = NULL,
  Topic11 = NULL,
  Topic12 = NULL,
  Topic13 = NULL,
  Topic14 = NULL,
  Topic15 = NULL
)
```

The output below shows, for each topic, the `r n` top words for each of the four topic-word evaluation metrics.

```{r echo=FALSE}
# top words for all topics
topic_words
```

A key task of topic analysis is to actually ascribe a meaning to the topics identified, i.e., labelling them. While this is clearly where human judgment should and does come into play, we attempt to conduct the labelling in a more stratetic (and thus less subjective) manner, following a 3-step procedure. This procedure is exemplified using topic `r topic_number`.

First, we consider the *words* contained in the topic, for instance by simply inspecting the top words (see output above). For a better visualization, we use a word cloud. As shown below, for a given topic (i.e., conditional upon a specific topic being chosen), it shows words weighted by their frequency. For instance, by judging at first sight topic `r topic_number` appears to be about right-wing nationalist issues, particularly immigration.

```{r echo=FALSE}
# word cloud
cloud(mod_prev, topic = topic_number, scale = c(2.0, 0.25))
```

Second, to get a more thorough insight into the topic, we take a look into actual *documents*, specifically into those showing the highest proportion for topic `r topic_number`.

```{r include=FALSE}
# actual labelling porcess

## (1) inspect most frequent words per topic
# cloud(mod_prev, topic = topic_number, scale = c(2.5, 0.25)) # word cloud
# topic_words$prob[topic_number,] # 20 most frequent words
# logbeta_matrix <- mod_prev$beta$logbeta[[1]]
# mod_prev$vocab[which.max(logbeta_matrix[topic_number,])] # most frequent word directly from (log)beta vector

## (2) evaluate most representative documents per topic
data_corpus$docname <- paste0(data_corpus$Twitter_Username, "_", data_corpus$Jahr, "_", data_corpus$Monat)

repr_docs <-  topic_props %>%
  arrange(desc(!!as.symbol(topic_number_long))) %>%
  .[1:docs_number, c("Name", "docname", "Datum", "Partei", "Bundesland", topic_number_long)] %>%
  left_join(data_corpus[,c("Tweets_Dokument", "docname")], 
            by = "docname")
```

For instance, the most representative document for topic `r topic_number`, with a proportion of `r scales::percent(repr_docs[topic_number_long][1,1], accuracy = 0.01)` is the one by MP `r repr_docs$Name[1]`, a member of the `r repr_docs$Partei[1]` party from `r repr_docs$Bundesland[1]`, from `r repr_docs$Datum[1]` which starts with:

```{r echo=FALSE, null_x, results = "asis"}
substr(repr_docs$Tweets_Dokument[1], 0, 256) # view most representative document
```

The second most representative document, still for topic `r topic_number`, has a proportion of `r scales::percent(repr_docs[topic_number_long][2,1], accuracy = 0.01)`. Its author is the same as for the first document, `r repr_docs$Name[2]`, but the date now is `r repr_docs$Datum[2]`. The document starts with:

```{r echo=FALSE, results = "asis"}
substr(repr_docs$Tweets_Dokument[2], 0, 255) # view 2nd most representative document
```

```{r echo=FALSE}
## (3) assign label
topic_labels[[topic_number]] <- "right/nationalist"
```

The documents confirm the first impression gained through top words and the word cloud: `r topic_number` concerns right-wing nationalist issues, in particular immigration. Thus, as a third step, we finally label the topic: in this case, as `r topic_labels[topic_number]`.

We repeat this 3-step procedure (inspecting top words and word cloud, reading through top documents, assigning a 1- or 2-word label) for all remaining topics, arriving at the following manual labels:.

```{r echo=FALSE}
## (3) assign label
topic_labels <- list(
  Topic1 = "right/nationalist",
  Topic2 = "miscellaneous_1",
  Topic3 = "green/climate",
  Topic4 = "social/housing",
  Topic5 = "Europe_english",
  Topic6 = "mobility",
  Topic7 = "Europe",
  Topic8 = "Corona",
  Topic9 = "left/anti-war",
  Topic10 = "Twitter/politics_1",
  Topic11 = "Twitter/politics_2",
  Topic12 = "miscellaneous_2",
  Topic13 = "Twitter/politics_3",
  Topic14 = "right-wing extremism",
  Topic15 = "social/health"
)
```

```{r echo=FALSE}
topic_labels %>%
  matrix(dimnames = list(names(topic_labels))) %>%
  knitr::kable()
```

## Global-level Topic Analysis

```{r include=FALSE}
doc_lengths <- lapply(data$documents[], length)
weights <- c()
i <- 1
while (i <= length(doc_lengths)) {
  weights[[i]] <- doc_lengths[[i]]/2
  i <- i + 1}
mean_weight <- mean(weights)
props_unweighted <- colMeans(mod_prev$theta[,1:K])
props_weighted <- colMeans((mod_prev$theta*weights/mean_weight)[, 1:K])

topic_labels_unlisted <- unlist(topic_labels, use.names = FALSE)

props_df <- data.frame(topic_labels_unlisted, props_unweighted, props_weighted) %>% reshape2::melt(id = "topic_labels_unlisted")
colnames(props_df) <- c("topic", "variable", "proportion")
```

Next, we identify two ways to calculate global topic proportions: either as the simple (unweighted) average of $\theta_d$ across all documents (i.e., as the average of MP-level proportions across all MPs); or by weighting each $\theta_d$ by the number of words in the respective documents, $N_d$. The table below shows all topics with their respective global proportions, for both weighting methodologies. We observe that for most topics, weighted and unweighted proportions are rather similar, but there are exceptions. In particular, the topics concerned with everyday political tweets have much higher unweighted than weighted frequencies; this makes sense, however, since such "diplomatic" tweets tend to be shorter than those which actually discuss a specific content.

```{r echo=FALSE}
ggplot(data = props_df, aes(x = topic, y = proportion, fill = variable)) + 
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("grey40","grey80")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1),
        axis.title.x = element_text(size = 12, face = "bold"),
        axis.title.y = element_text(size = 12, face = "bold"))
```

While labelling tells us which words best represent each topic - and thus, what each topic truly represents - it does not yet tell us to which extent individual topics are related to each other. In the graph below, we visualize the similarity of two topics, Topic 3 (`r topic_labels[3]`) and Topic 6 (`r topic_labels[6]`), in terms of their vocabulary usage. As suggested by the topic labels already, there is a significant overlap in vocabulary usage.

```{r echo=FALSE}
# vocabulary usage comparison for two topics
plot(mod_prev, type = "perspectives", topics = c(3, 6), n = 30)
```

More generally, we can evaluate the connectedness between different topics with the topic correlation matrix of the correlations between document-level topic proportions $\theta_d$. This is visualized in the graph below.

```{r echo=FALSE}
# global topic correlation
cormat <- cor(mod_prev$theta)
ggcorrplot::ggcorrplot(cormat) +
  scale_x_continuous(breaks = seq(1, 15, by = 1)) +
  scale_y_continuous(breaks = seq(1, 15, by = 1)) +
  labs(x = "topic number", y = "topic number") +
  theme_minimal() +
    theme(axis.title.x = element_text(size = 12, face = "bold"),
          axis.title.y = element_text(size = 12, face = "bold"))
```

Most topics are negatively correlated with each other, which does not come as a surprise, given the relatively low total number of topics, `r K`, and that topic proportions are "supplements": the higher one topic proportion, the lower the total of the others. Moreover, most topic correlations are rather weak in absolute size: the strongest negative  correlation (`r scales::percent(cormat[1,7], accuracy = 0.01)`)is the one between  topic  1 (`r topic_labels[[1]]`) and topic 7 (`r topic_labels[[1]]`), while the strongest positive correlation (`r scales::percent(cormat[3,6], accuracy = 0.01)`) is the one shown before, between (`r topic_labels[3]`) and (`r topic_labels[6]`).

We can also visualize these correlations using a network graph, where topics are connected whenever they are positively correlated. Most topics are only related to two other topics, while none are related to more than three. The only "isolated" topic is topic 8, `r topic_labels[8]`, which makes sense since it only entered the public sphere in early 2020, i.e., during the last months of our data collection period. In general, the relationships between the topics, as depicted below, are very much in line with their labelling.

```{r echo=FALSE}
set.seed(111)
mod_prev_corr <- topicCorr(mod_prev, method = "simple", cutoff = 0.00,
                           verbose = TRUE) # based on correlations between mod_prev$theta
vertex_sizes <- rep(20, K)
plot.topicCorr(mod_prev_corr, vlabels = topic_labels, vertex.label.cex = 1, vertex.size = vertex_sizes)
```

## Covariate-level Topic Analysis

After this analysis of topics at a global level, in particular of their labeling and proportions, we now proceed to analyze metadata information (i.e., document-level covariates) and its impact on topic proportions. As mentioned before, the covariates included are party, state (both categorical), date (smooth effect), percentage of immigrants, GDP per capita, unemployment rate, and the 2017 vote share (the last four as smooth effects, on an electoral-district level). Since the target variable $\theta_{(k)}$ is not observable and being estimated itself during the estimation of the STM, we recur to the method of composition to account for the uncertainty contained within $\theta_{(k)}$.

### Method of Composition

Let $\theta_{(k)} \in [0,1]^{D}$ denote the proportions of the $k$-th topic for all $D$ documents. Suppose that we want to perform a regression of these topic proportions $\theta_{(k)}$ on a subset $\tilde{X} \in \mathbb{R}^{D \times \tilde{P}}$ of prevalence covariates $X$. The true topic proportions are unknown, but the STM produces an estimate of the approximate posterior of $\theta_{(k)}$, $q(\theta_{(k)} | \Gamma, \Sigma, X)$, where $\Gamma := \Gamma(w,X,Y)$ and $\Sigma := \Sigma(w,X,Y)$. A naïve approach would be to regress the estimated mode of the approximate posterior distribution on $\tilde{X}$. However, this approach neglects much of the information contained in the distribution. Instead, sampling $\theta_{(k)}^*$ from the posterior distribution, performing a regression for each sampled $\theta_{(k)}^*$ on $\tilde{X}$, and then sampling from the estimated distributions of regression coefficients, provides an i.i.d.\ sample from the marginal posterior distribution of regression coefficients. This procedure is known as the method of composition in the social sciences [@tanner2012tools, p.52].

Formally, let $\xi$ denote the regression coefficients from a regression of $\theta_{(k)}$ on $\tilde{X}$, and let $q(\xi| \theta_{(k)}, \tilde{X})$ be the approximate posterior distribution of these coefficients, i.e.\ given design matrix $\tilde{X}$ and response $\theta_{(k)}$.

The R package *stm* implements a simple OLS regression through its *estimateEffect* function. Using this framework we frequently observe predicted proportions outside of $(0,1)$, given that the restricted domain of $\theta_{(k)}$ is not taken into account. Moreover, credible intervals are non-informative, due to violated model assumptions. Therefore, to adequately model the topic proportions we perform a beta regression (with logit-link), since the sampled proportions are restricted to the interval $(0,1)$. More information on why beta regression is useful in such a scenario can be found in @ferrari2004beta. In case of a beta regression, $q(\xi| \theta_{(k)}, \tilde{X})$ is a normal distribution (see e.g.\ @ferrari2004beta, p.\ 17). 

The method of composition can now be described by repeating the following process $m$ times:
\begin{enumerate}
\item Draw $\theta_{(k)}^* \sim q(\theta_{(k)} | \Gamma, \Sigma, X)$.
\item Draw $\xi^* \sim q(\xi| \theta_{(k)}^*, \tilde{X})$.
\end{enumerate}

Then, $\xi_1^*, \dots, \xi_m^*$ is an i.i.d.\ sample from the marginal posterior
\begin{align*}
q(\xi | \Gamma, \Sigma, X) := \int_{\theta_{(k)}} q(\xi| \theta_{(k)}, \tilde{X}) q(\theta_{(k)} | \Gamma, \Sigma, X) \text{d} \theta_{(k)} = \int_{\theta_{(k)}} q(\xi, \theta_{(k)} | \Gamma, \Sigma, X) \text{d} \theta_{(k)}, 
\end{align*}

where $q(\xi, \theta_{(k)} | \Gamma, \Sigma, X) := q(\xi| \theta_{(k)}, \tilde{X}) q(\theta_{(k)} | \Gamma, \Sigma, X)$. Thus, by integrating over $\theta_{(k)}$, this approach allows incorporating uncertainty about $\theta_{(k)}$ when determining $\xi$.

### Visualization

```{r echo=FALSE}
# factorize categorical variables, set CDU/CSU as reference category for variable "Partei"
data$meta$Partei <- data$meta$Partei %>%
  as.factor() %>%
  relevel(ref = 3)
data$meta$Bundesland <- as.factor(data$meta$Bundesland)

# prep <- stm::estimateEffect(
#   1:15 ~ s(t, df=20),
#   mod_prev,
#   #documents = data$documents,
#   metadata = data$meta,
#   uncertainty = "Global"
# )
# summary(prep, topics = 1)
# 
# par(mfrow=c(3,3))
# for (i in 1:9){
#   plot(prep, "t", method = "continuous", topics = i, 
#        main = paste0(mod_labels[i,], collapse = ", "), 
#        printlegend = F, xlab = "t")
# }
# par(mfrow=c(3,3))
# for (i in 1:9){
#   plot(prep, "Partei", method = "pointestimate", topics = i, labeltype = "custom",
#        custom.labels = c("CDU/CSU", "FDP", "Die Linke", "SPD", "Bündnis 90/Die Grünen", "AfD"), 
#        main = paste0(mod_labels[i,], collapse = ", "), 
#        printlegend = F, xlab = "Expected Topic Proportion")
# }

# ----------------------------------------------------------------------------------------------

library("betareg")
library("mvtnorm")

# ------------------------------- Create helper functions --------------------------------------

sigmoid <- function(x) exp(x)/(1+exp(x))

majority <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

sample_normal <- function(mod) {
  mu <- mod$coefficients$mean
  var <- mod$vcov[1:length(mu), 1:length(mu)]
  mvtnorm::rmvnorm(1, mean = mu, sigma = var)
}

sample_coefs_beta <- function(stmobj, formula, metadata, nsims = 25, seed = NULL){
  topic_n <- as.numeric(as.character(formula)[2])
  topic_nam <- paste0("Topic", topic_n)
  set.seed(seed)
  theta_sim <- do.call(rbind, stm::thetaPosterior(stmobj, nsims = nsims, type = "Global"))[,topic_n]
  theta_sim <- lapply(split(1:(length(theta_sim)), 1:nsims), 
                      function(i) setNames(data.frame(theta_sim[i]), topic_nam))
  f <- paste(topic_nam, "~", as.character(formula)[3])
  est_beta <- lapply(theta_sim, 
                     function(x) betareg::betareg(as.formula(f), data = cbind(x, metadata)))
  res <- lapply(est_beta, sample_normal)
  return(res)
}

sample_all_betas <- function(covar, metadata, nsims, topics = K, seed = NULL) {
  res <- vector(mode = "list", length = topics)
  for (topic in 1:topics) {
    outcome <- topic
    formula <- as.formula(paste(outcome, covar, sep = "~"))
    res[[topic]] <- sample_coefs_beta(mod_prev, formula, metadata = metadata, nsims, seed = seed)
  }
  return(res)
}

predict_props_beta <- function(beta_coefs, est_var, formula, metadata){
  dat <- metadata[, -which(names(metadata) == est_var)]
  dat <- lapply(dat, function(x) if(is.numeric(x)) median(x) else majority(x))
  if (is.numeric(metadata[,est_var])) {
    dat_fit <- data.frame(
      dat, fitvar = seq(min(metadata[,est_var]), max(metadata[,est_var]), length.out = 500)
    )
  } else {
    dat_fit <- data.frame(dat, fitvar = unique(metadata[,est_var]))
  }
  names(dat_fit) <- c(names(dat),est_var)
  f <- paste("~",as.character(formula)[3])
  xmat <- stm::makeDesignMatrix(as.formula(f), metadata, dat_fit)
  fit_vals <- do.call(cbind, lapply(beta_coefs, function(x) sigmoid(xmat %*% t(x))))
  mu <- quanteda::rowMeans(fit_vals)
  ci <- apply(fit_vals, 1, function(x) quantile(x, probs = c(0.025, 0.975)))
  res <- data.frame(dat_fit[[est_var]], mu, ci[1,], ci[2,])
  names(res) <- c(est_var, "proportion", "ci_lower", "ci_upper")
  return(res)
}

# ----------------------------------------------------------------------------------------------

# ----------------------------- Actual Model Prediction ----------------------------------------

# covariates
covar <- "Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) + 
  s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)"
# formula always in form "i~var1+var2+...", where i = topic number
topic_number <- 1
formula <- as.formula(paste(topic_number, covar, sep = "~"))
# obtain list of nsims beta regression outputs
nsims <- 100

# beta_coefs <- sample_coefs_beta(mod_prev, formula, data$meta, nsims = 25) # for topic = outcome only

# set.seed(123)
# all_betas <- sample_all_betas(covar = varlist, metadata = data$meta, nsims = nsims, topics = K)
# saveRDS(all_betas, "./data/all_betas.rds")

all_betas <- readRDS("../data/all_betas.rds")

# estimate effect for variable while other variables held as median/majority value
# preds <- predict_props_beta(beta_coefs, "t", formula, data$meta)

# example plots
# par(mfrow=c(3,3))
# for (v in c("t", "Partei", "Bundesland", "Struktur_4", "Struktur_22", "Struktur_42", "Struktur_54")) {
#   plot(predict_props_beta(all_betas[[topic_number]], v, formula, data$meta)[,1:2], type = "l", col = "blue")
# }
```

```{r echo=FALSE}
library(grid)
library(gridExtra)
library(scales)

covar <- "Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) + 
  s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)"
varlist <- c(
  "t", "Partei", "Bundesland", "Struktur_4", "Struktur_22", "Struktur_42", "Struktur_54"
)
varlist_fullnames <- c(
  "Time", "Party", "Federal State", "Immigrants (%)", "GDP per capita", 
  "Unemployement Rate (%)", "vote share (%)"
)
```

We can now apply the method of composition, based on a beta regression, in order to quantify covariate effects. Setting the number of simulations to `r nsims`, we thus sample $\xi^*_1, \dots, \xi^*_{100}$ from the approximate posterior distribution $q(\xi | \Gamma, \Sigma, X)$. In order to plot the predicted effects, we input $\tilde{X}\xi^*$ into the sigmoid function, which is the response function corresponding to a beta regression with logit-link, and calculate the predicted proportions. When visualizing the impact of a particular covariate, all other covariates are held at their median, in line with the methodology employed in the *stm* package.

We discuss the impact of covariates on topic proportions for topics 3 (`r topic_labels[[3]]`) and 4 (`r topic_labels[[4]]`), sub-dividing the analysis into smooth effects (time, immigration, GDP, and unemployment) and categorical variables (party and state). For smooth effects, it is important to recall that their borders are inherently unstable, which is why one should refrain from (over-)interpreting them. For both continuous and categorical variables, black lines indicate the *mean*, the shaded area represents 95% credible intervals.

By looking at the smooth effects for topic 3 below, we find that its proportion increases over time until the 25th month, corresponding to September 2019, decreasing sharply afterwards. However this sharp decline is to be taken with caution due to the instability of splines at the borders of the covariate domain. Note that the absolute changes in topic proportions over time for the `r topic_labels[[3]]` topic are rather small (around 4%). The effect of immigrants (as percentage of the total population) is negative across the entire domain, and rather steadily so. The impact of GDP per capita on topic 3 is unclear/constant, while unemployment rate show an overall positive effect.

```{r echo=FALSE}
## Topic 3: Green/Climate
formula_3 <- 3~Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) + 
  s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)
beta_coefs_3 <- all_betas[[3]]
preds_varlist_3 <- lapply(varlist, function(v) predict_props_beta(beta_coefs_3, v, formula_3, data$meta))
names(preds_varlist_3) <- varlist

### Continuous Plots
for(v in setdiff(varlist, c("Partei", "Bundesland"))){
  plot_nam <- paste0("plot_", v)
  assign(plot_nam, ggplot(preds_varlist_3[[v]], aes(!!as.symbol(v))) + 
           geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), fill = "grey70") +
           xlab(varlist_fullnames[varlist==v]) +
           ylab("Expected Topic Proportion") +
           geom_line(aes(y = proportion)) +
           scale_x_continuous(labels = scales::comma))
}
gridExtra::grid.arrange(
  plot_t, plot_Struktur_4, plot_Struktur_22, plot_Struktur_42, ncol=2, 
  top = grid::textGrob("Topic 3: Green/Climate", gp=grid::gpar(fontsize=16, fontface = "bold"))
)
```

Regarding the effect of categorical variables on topic `r topic_labels[[3]]`, we consider the political party, arguably the most decisive covariate. As was to be expected, we find high topic prevalence for the green party, yet the liberal party is, somewhat surprisingly, the party with the highest prevalence. Similar to the smooth effects, total variation in topic proportions across parties amounts to approximately 8%, as can be seen in the graph below.

```{r echo=FALSE}
### Categorial Plots
(plot_party_3 <- ggplot(preds_varlist_3$Partei, aes(y=proportion, x = Partei)) +
  geom_crossbar(aes(ymax = ci_upper, ymin = ci_lower), fill = "grey70") +
  xlab("Party") +
  ylab("Expected Topic Proportion") +
  ggtitle("Topic 3: Green/Climate")+
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5)))
```

As for topic 4, `r topic_labels[[4]]`, we observe that most (quasi-)continuous variables have a small effect in absolute terms: the absolute variation in topic proportion across the covariate domains merely amounts to 4%, compared to around 8% for the `r topic_labels[[3]]` topic. The time effect is similar to the one for topic 3, particularly the decreasing topic prevalence since September 2019. For the other variables, no clear effect is discernible.

```{r echo=FALSE}
## Topic 4: Social/Housing
formula_4 <- 4~Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) + 
  s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)
beta_coefs_4 <- all_betas[[4]]
preds_varlist_4 <- lapply(varlist, 
                          function(v) predict_props_beta(beta_coefs_4, v, formula_4, data$meta))
names(preds_varlist_4) <- varlist

### Continuous Plots
for(v in setdiff(varlist, c("Partei", "Bundesland"))){
  plot_nam <- paste0("plot_", v)
  assign(plot_nam, ggplot(preds_varlist_4[[v]], aes(!!as.symbol(v))) + 
    geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), fill = "grey70") +
    ylab("Expected Topic Proportion") +
    xlab(varlist_fullnames[varlist==v]) +
    geom_line(aes(y = proportion)) +
    scale_x_continuous(labels = scales::comma))
}
gridExtra::grid.arrange(
  plot_t, plot_Struktur_4, plot_Struktur_22, plot_Struktur_42, ncol=2, 
  top = grid::textGrob("Topic 4: Social/Housing", gp=grid::gpar(fontsize=16, fontface = "bold"))
)
```

The effect of political party on the relevance assigned to the `r topic_labels[[4]]` topic is very much in line with a priori expectations: the left party and social democrats have the highest topic prevalence, at around 10%, the nationalist party the lowest one at 5%. The overall effect of covariate party is thus similar for topics `r topic_labels[[3]]` and `r topic_labels[[4]]`.

```{r echo=FALSE}
### Categorial Plots
(plot_party_4 <- ggplot(preds_varlist_4$Partei, aes(y=proportion, x = Partei)) +
  geom_crossbar(aes(ymax = ci_upper, ymin = ci_lower), fill = "grey70") +
  xlab("Party") +
  ylab("Expected Topic Proportion") +
  ggtitle("Topic 4: Social/Housing")+
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5)))
```

Finally, the graph below shows a summary comparison of topic prevalence across all parties, for topics `r topic_labels[[1]]`, `r topic_labels[[3]]`, and `r topic_labels[[4]]`. The results are generally consistent with expectations. The proportions of topics `r topic_labels[[3]]`and `r topic_labels[[4]]` vary between 4% and 12% and between 5% and 10%, respectively. For topic 1,  `r topic_labels[[1]]`, note how topic prevalence for the AfD party amounts to more than 40%, implying that more than 40% of the total content tweeted by AfD party members is about right-wing/nationalist issues, particulary immigration; for all other parties, topic 1 is rather marginal at 3-4%.

```{r echo=FALSE}
## Topic 1: Right/Nationalist
formula_1 <- 1~Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) + 
  s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)
beta_coefs_1 <- all_betas[[1]]
preds_varlist_1 <- lapply(varlist, 
                          function(v) predict_props_beta(beta_coefs_1, v, formula_1, data$meta))
names(preds_varlist_1) <- varlist

# ---------

# gridExtra::grid.arrange(plot_party_3, plot_party_4)

preds_varlist_1$Partei$Topic <- "Right/Nationalist"
preds_varlist_3$Partei$Topic <- "Green/Climate"
preds_varlist_4$Partei$Topic <- "Social/Housing"
party_data <- rbind(preds_varlist_1$Partei, preds_varlist_3$Partei, preds_varlist_4$Partei)
(plot_party <- ggplot(party_data, aes(y=proportion, x = Partei, fill = Topic)) +
    geom_col(position = "dodge") +
    scale_fill_manual(values=c("green", "blue", "red")) +
    xlab("Party") +
    ylab("Expected Topic Proportion") +
    ggtitle("Topic Proportions by Party")+
    theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5)))
```

## Train-Test-Split

* causal inference within a topic analysis setting (see @egami2018make)
* "predictive power" of covariates

# Conclusion

TBD

# Bibliography

---
title: 'Draft May 2020'
author: "Simon"
date: "May 2020"
output:
  pdf_document: default
  html_document: default
bibliography: bibliography.bib
biblio-style: myabbrvnat
link-citations: yes
linkcolor: blue
header-includes:
  - \usepackage{caption}
  - \usepackage{float}
  - \captionsetup{labelformat=empty}
  - \usepackage{multirow}
  - \usepackage{graphicx}
  - \usepackage{booktabs}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Preparation, include=FALSE}
# ----------------------------------------------------------------------------------------------
# ---------------------------------------- Preparation -----------------------------------------
# ----------------------------------------------------------------------------------------------

# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
  "betareg", "grid", "gridExtra", "huge", "mvtnorm", "quanteda", "reshape2", "scales", "stm", "stringi", "tidyverse", "tm"
)
not_installed <- packages_required[!packages_required %in%
                                     installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
  lapply(
    not_installed,
    install.packages,
    repos = "http://cran.us.r-project.org",
    dependencies = TRUE,
    type = itype
  )
}
lapply(packages_required, library, character.only = TRUE)

# set working directory
# setwd('C:\\Users\\Simon\\OneDrive\\Uni\\LMU\\SS 2020\\Statistisches Consulting\\Bundestag-MP-Analyse')
# setwd("/Users/patrickschulze/Desktop/Consulting/Bundestag-MP-Analyse")
```

### Data

```{r echo=FALSE, include=FALSE}
# load twitter data and aggregate on a per-user basis
topic <- readRDS("../data/topic.rds")
prep <- readRDS("../data/prep.rds")

topic_user <- topic %>% 
  group_by(Name) %>% 
  mutate(Tweets_Dokument = paste(Tweets, collapse = ' ')) %>%
  summarize(
    Twitter_Username = max(Twitter_Username), 
    Tweets_Dokument = max(Tweets_Dokument),
    Anzahl_Follower = max(Anzahl_Follower)
  ) %>%
  ungroup()

# merge with personal and socioeconomic data
abg_df <- read_delim("../data/abg_df.csv", delim = ",") %>%
  rename(Twitter_Username = Twitter, Wahlkreis_Nr = `Wahlkreis-Nr.`)
se_df <- read_delim("../data/se_df.csv", delim = ",") %>% 
  rename(Wahlkreis_Nr = `Wahlkreis-Nr.`, "AfD Anteil" = "AFD Anteil") %>% 
  select(-Bundesland)

alldata <- topic %>% 
  inner_join(abg_df) %>% 
  inner_join(se_df, by = "Wahlkreis_Nr")

alldata <- alldata %>% 
  filter(Partei != "fraktionslos")

```

As a first step towards applying the STM to German political entities, we constructed a database with personal information about all German MPs. Using Python's BeautifulSoup web scraping tool as well as a selenium webdriver, we gathered data such as name, party, and electoral district from the [official parliament website](https://www.bundestag.de/abgeordnete) for all of the 709 members of the German parliament during its 19th election period, elected on September 24, 2017. (Footnote: MPs who resigned or passed away since this date were also listed on the website and thus included initially; they were manually excluded from further analysis.)

Since information on social media profiles was scarce and incomplete on the official parliament website, we scraped official party homepages for each of the six political parties represented in the current parliament. MPs who did not provide a Twitter account either on the official parliament website or on their party's official homepage were excluded. Using Python's tweepy library to access the official Twitter API, we scraped all tweets by German MPs from September 24, 2017 through April 24, 2020, i.e., during a total of 31 months. (Footnote: tweepy restricts the total number of retrievable tweets to 3,200. For those MPs with a larger number of tweets, the most recent 3,200 tweets are taken into account. However, this only affects two MPs.) This initially yielded `r nrow(topic)` tweets from a total of `r nrow(topic_user)` members of parliament.

To complement personal data, we also gathered socioeconomic data such as GDP per capita and unemployment rate as well as 2017 election results on an electoral-district level for all of the 299 electoral districts, from the [official electoral website](https://www.bundeswahlleiter.de). After removing  independent MPs as well as MPs without a specific electoral district assigned to them (for matchability with socioeconomic data), the final dataset counted `r nrow(prep)` MPs. The corresponding total number of tweets amounted to `r nrow(alldata)`. The table below shows total monthly tweet frequencies for our period of analysis, September 24, 2017 through April 24, 2020. As can be seen, tweet frequencies - though fluctuating - increase over time, peaking at almost 20,000 in March 2020.

```{r echo=FALSE, fig.align='center', fig.asp=.6, fig.width=10, warning=FALSE, out.width='82%'}

data_adj <- alldata %>% mutate(Jahr = lubridate::year(Datum), Monat = lubridate::month(Datum))
data_adj$date <- with(data_adj, sprintf("%d-%02d", Jahr, Monat))
ggplot(data_adj, aes(x = date)) + geom_histogram(stat = "count") +
  labs(x = "time", y = "# tweets")  +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1),
        axis.title.x = element_text(size = 12, face = "bold"),
        axis.title.y = element_text(size = 12, face = "bold"))
```

Next, data were grouped and tweets concatenated on a per-user level (thus aggregating tweets across the entire 31 months) as well as on a per-user per-month level, yielding a user-level and a (user-level) monthly dataset. This means that a document represents the concatenation of *all* of a single MP's tweets for the user-level dataset and a single MP's *monthly* tweets for the monthly dataset. This also means that MP-level metadata such as personal information and socioeconomic data (through the electoral district matching) can be used as document-level covariates. For the monthly dataset, the temporal component (year and month) constitutes an additional covariate. At this point, the data preparation was completed, thus marking the starting point of the preprocessing required for topic analysis, which is identical for both datasets.

We used the quanteda package within the R programming language for preprocessing. As a first step, we built a quanteda corpus from all documents, already transcribing German umlauts *ä/Ä*, *ö/Ö*, *ü/Ü* as well as German ligature *ß* as *ae/Ae*, *oe/Oe*, *ue/Ue*, and *ss* and removed hyphens. Next, we transformed the text data into a quanteda document-feature matrix (DFM), which essentially tokenizes texts, thereby convering all characters to lowercase. From the DFM, we removed an extensive list of German stopwords, using the [stopwords-iso GitHub repository](https://github.com/stopwords-iso/stopwords-iso), as well as English stopwords included in the quanteda package. Moreover, hashtags, usernames, quantities and units (e.g., *10kg* or *14.15uhr*), interjections (e.g., *aaahhh* or *ufff*), terms containing non-alphanumerical characters, meaningless word stumps (e.g., *innen* from the German female plural declension or *amp*, the remainder left after removing the ampersand sign, *&*) were removed. Terms with less than four characters and terms with a term frequency (overall number of occurrences) below five or with a document frequency (number of documents containing the word) below three were excluded. Finally, we manually removed over-frequent terms that would diminish the distinguishability of topics, such as *bundestag* or *polit*. 

We also performed word-stemming, which means cutting off word endings to remove discrepancies arising purely from declensions or conjugations - of particular importance for the German language. Due to the nature of the German language, the additional gains of lemmatization (which aims at identifying the base form of each word) would only be small as compared to the large increase in complexity, which is why we decided to use stemming only. Another issue when dealing with German language documents are compound words, which are sometimes hyphenated, basically leading to a distinction where semantically there is none. We addressed this issue by removing hyphens in the very beginning of the preprocessing and converting all terms to lowercase, thus "gluing together" compound words; this way, terms like *Bundesregierung* and *Bundes-Regierung* are both transformed into *bundesregierung* (and, after stemming, into *bundesregier*). Finally, automatic segmentation techniques were not necesssary for the German language (@lucas2015computer).

```{r include=FALSE}
# load data
data <- readRDS("../data/preprocessed_monthly.rds")
colnames_table <- read.csv(file = "../data/topic_monthly_colnames.csv")
data_corpus <- readRDS("../data/prep_monthly.rds")

data_aggregated <- readRDS("../data/preprocessed.rds") # MP-level (non-monthly) data
```

As the result of preprocessing, one empty MP-level document was dropped, so that a total of `r nrow(data$meta)` MP-level documents were eventually analyzed, each one associated with `r ncol(data$meta)` covariates.

### Results

```{r include=FALSE}
# # search hyperparameter space for optimal K
# hyperparameter_search <- searchK(
#   documents = data$documents,
#   vocab = data$vocab,
#   data = data$meta,
#   # K = c(5,6,7,8,9,10),
#   K = c(5,10,15,20,25,30,35,40),
#   prevalence =~ Partei,
#   max.em.its = 150,
#   init.type = "Spectral"
# )

# load searchK results
searchK_data <- readRDS("../data/searchK_large_data.rds")
```

Throughout the topic analysis, we use the stm package, which is implemented in the R programming language. The STM has a single hyperparameter K, the number of topics. While there is no *true* or *optimal* number of topics, we explore the hyperparameter space using the searchK function to get to get an understanding of the impact of K on model fit. We use four of the metrics that come with this function, *held-out likelihood*, *semantic coherence*, *exclusivity*, and *residuals*. As for the first one, the searchK function randomly holds out a proportion of some of the documents. This set of held-out words is then used to evaluate their probability given the trained model, giving rise to the *held-out likelihood*. Regarding the second metric, a model with K topics is *semantically coherent* whenever those words that characterize a specific topic (i.e., the most frequent words within a topic) also do appear in the same documents. *Exclusivity* basically tells us to which degree a topic's word *only* occur in that topic (for more detail, see the *FREX* methodology further below). Finally, *residuals* is a metric based on residual dispersion, which theoretically should be equal to one; so if the observed residuals exceed this value, the number of topics was most likely chosen *insufficiently small*.

Another aspect to be taken into account when choosing K (or, to be precise, when choosing a search grid for searchK) is interpretability. While a large K certainly allows for a more fine-grained determination of topics, the resulting topics might be rather hard to label. Furthermore, for large K we might get many topics which themselves could easily be considered sub-topics of those topics that we would get when using a smaller value for K. The graphs below shows the four metrics, as introduced above, for values of K between 5 and 40 (in steps of 5).

```{r echo=FALSE}
plot_heldout <- ggplot(data = searchK_data$results, aes(x = K, y = heldout)) +
                  geom_line() +
                  geom_point() +
                  labs(y = "held-out likelihood") +
                  theme_minimal() +
                  theme(axis.text.x = element_text(vjust = 1, size = 12, hjust = 1),
                        axis.title.x = element_text(size = 12, face = "bold"),
                        axis.title.y = element_text(size = 12, face = "bold"))

plot_semcoh <- ggplot(data = searchK_data$results, aes(x = K, y = semcoh)) + 
                  geom_line() +
                  geom_point() +
                  labs(y = "semantic coherence") +
                  theme_minimal() +
                  theme(axis.text.x = element_text(vjust = 1, size = 12, hjust = 1),
                        axis.title.x = element_text(size = 12, face = "bold"),
                        axis.title.y = element_text(size = 12, face = "bold"))

plot_exclus <- ggplot(data = searchK_data$results, aes(x = K, y = exclus)) +
                  geom_line() +
                  geom_point() +
                  labs(y = "exclusivity") +
                  theme_minimal() +
                  theme(axis.text.x = element_text(vjust = 1, size = 12, hjust = 1),
                        axis.title.x = element_text(size = 12, face = "bold"),
                        axis.title.y = element_text(size = 12, face = "bold"))

plot_residual <- ggplot(data = searchK_data$results, aes(x = K, y = residual)) + 
                  geom_line() +
                  geom_point() +
                  labs(y = "residuals") +
                  theme_minimal() +
                  theme(axis.text.x = element_text(vjust = 1, size = 12, hjust = 1),
                        axis.title.x = element_text(size = 12, face = "bold"),
                        axis.title.y = element_text(size = 12, face = "bold"))

gridExtra::grid.arrange(plot_heldout, plot_semcoh, plot_exclus, plot_residual, ncol=2)

K <- 15
```

Both 15 and 20 topics seem to be good trade-offs between the metrics used. Taking into account the interpretability aspect, we opt for K = `r K`.

Before fitting the model, we need to choose the document-level covariates we want to include. Since a topic model is explorative by definition, we simply include those covariates that seem to be most influential *a priori*: party and state (both categorical), date (as smooth effect), as well as percentage of immigrants, GDP per capita, unemployment rate, and the 2017 election results of the MP's respective party (the last four as smooth effects and on an electoral-district level).

```{r echo=FALSE}
# choose covariates and number of topics
prev_var <- c("Partei", "Bundesland", "Datum", "Struktur_4", "Struktur_22", "Struktur_42", "Struktur_54")
outcome <- ""
prevalence <- as.formula(paste(outcome, paste(prev_var, collapse = "+"), sep = "~")) 

# # fit model
# mod_prev <- stm::stm(
#   documents = data$documents,
#   vocab = data$vocab,
#   data = data$meta,
#   K = K,
#   prevalence = prevalence,
#   # gamma.prior = 'L1',
#   seed = 123,
#   max.em.its = 200,
#   init.type = "Spectral")

mod_prev <- readRDS("../data/mod_prev_monthly.rds")
```

The first thing to do after fitting the model is to (visually) inspect the topics, in particular their best words, as demonstrated in the output below. There are different metrics for evaluating which words are most representative of a topic. The STM comes with four such metrics: *highest probability*, *FREX*, *Lift*, and *Score*. The first one, *highest probability*, simply outputs for each topic those words in the topic-specific word vector, $\beta_{d,n}$, with the highest corpus frequency, i.e, those with the highest absolute frequency across all documents. *FREX* takes into account not only how frequent but also how exclusive words are: for a given topic, it is calculated by taking the harmonic mean of i) the word's rank by probability within the topic (frequency) and ii) the topic's rank by  word frequency  across all topics (exclusivity). Further information on the estimation of *FREX*, *Lift*, and *Score* can be found in @bischof2012summarizing, in the [lda package by Jonathan Chang (2015)](https://www.rdocumentation.org/packages/lda/versions/1.4.2/topics/lda-package), and in @taddy2012estimation, respectively.

```{r echo=FALSE}
# labeling workflow (for each topic): 

## (1) inspect most frequent words per topic (using different metrics as well as word cloud)
## (2) evaluate most representative documents per topic
## (3) assign label

# first, prepare objects/variables needed for labelling process

## table of MAP topic proportions per document (for all topics)
topic_props <- make.dt(
  mod_prev, 
  data$meta[c("Name", "Partei", "Datum", "Bundesland")]) %>% 
  cbind(docname = names(data$documents), .)

## top words per topic (for all topics)
n <- 5
topic_words <- labelTopics(mod_prev, n = n)

## topic to be evaluated
topic_number <- 1
topic_number_long <- paste0("Topic", topic_number)

## number of top documents to be printed in step (2)
docs_number <- 10

## initialize list with empty labels
topic_labels <- list(
  Topic1 = NULL,
  Topic2 = NULL,
  Topic3 = NULL,
  Topic4 = NULL,
  Topic5 = NULL,
  Topic6 = NULL,
  Topic7 = NULL,
  Topic8 = NULL,
  Topic9 = NULL,
  Topic10 = NULL,
  Topic11 = NULL,
  Topic12 = NULL,
  Topic13 = NULL,
  Topic14 = NULL,
  Topic15 = NULL
)
```

The output below shows, for each topic, the `r n` top words for each of the four topic-word evaluation metrics.

```{r echo=FALSE}
# top words for all topics
topic_words
```

A key task of topic analysis is to actually ascribe a meaning to the topics identified, i.e., labelling them. While this is clearly where human judgment should and does come into play, we attempt to conduct the labelling in a more stratetic (and thus less subjective) manner, following a 3-step procedure. This procedure is exemplified using topic `r topic_number`.

First, we consider the *words* contained in the topic, for instance by simply inspecting the top words (see output above). For a better visualization, we use a word cloud. As shown below, for a given topic (i.e., conditional upon a specific topic being chosen), it shows words weighted by their frequency. For instance, by judging at first sight topic `r topic_number` appears to be about right-wing nationalist issues, particularly immigration.

```{r echo=FALSE}
# word cloud
cloud(mod_prev, topic = topic_number, scale = c(2.0, 0.25))
```

Second, to get a more thorough insight into the topic, we take a look into actual *documents*, specifically into those showing the highest proportion for topic `r topic_number`.

```{r include=FALSE}
# actual labelling porcess

## (1) inspect most frequent words per topic
# cloud(mod_prev, topic = topic_number, scale = c(2.5, 0.25)) # word cloud
# topic_words$prob[topic_number,] # 20 most frequent words
# logbeta_matrix <- mod_prev$beta$logbeta[[1]]
# mod_prev$vocab[which.max(logbeta_matrix[topic_number,])] # most frequent word directly from (log)beta vector

## (2) evaluate most representative documents per topic
data_corpus$docname <- paste0(data_corpus$Twitter_Username, "_", data_corpus$Jahr, "_", data_corpus$Monat)

repr_docs <-  topic_props %>%
  arrange(desc(!!as.symbol(topic_number_long))) %>%
  .[1:docs_number, c("Name", "docname", "Datum", "Partei", "Bundesland", topic_number_long)] %>%
  left_join(data_corpus[,c("Tweets_Dokument", "docname")], 
            by = "docname")
```

For instance, the most representative document for topic `r topic_number`, with a proportion of `r scales::percent(repr_docs[topic_number_long][1,1], accuracy = 0.01)` is the one by MP `r repr_docs$Name[1]`, a member of the `r repr_docs$Partei[1]` party from `r repr_docs$Bundesland[1]`, from `r repr_docs$Datum[1]` which starts with:

```{r echo=FALSE, null_x, results = "asis"}
substr(repr_docs$Tweets_Dokument[1], 0, 256) # view most representative document
```

The second most representative document, still for topic `r topic_number`, has a proportion of `r scales::percent(repr_docs[topic_number_long][2,1], accuracy = 0.01)`. Its author is the same as for the first document, `r repr_docs$Name[2]`, but the date now is `r repr_docs$Datum[2]`. The document starts with:

```{r echo=FALSE, results = "asis"}
substr(repr_docs$Tweets_Dokument[2], 0, 255) # view 2nd most representative document
```

```{r echo=FALSE}
## (3) assign label
topic_labels[[topic_number]] <- "right/nationalist"
```

The documents confirm the first impression gained through top words and the word cloud: `r topic_number` concerns right-wing nationalist issues, in particular immigration. Thus, as a third step, we finally label the topic: in this case, as `r topic_labels[topic_number]`.

We repeat this 3-step procedure (inspecting top words and word cloud, reading through top documents, assigning a 1- or 2-word label) for all remaining topics, arriving at the following manual labels:.

```{r echo=FALSE}
## (3) assign label
topic_labels <- list(
  Topic1 = "right/nationalist",
  Topic2 = "miscellaneous_1",
  Topic3 = "green/climate",
  Topic4 = "social/housing",
  Topic5 = "Europe_english",
  Topic6 = "mobility",
  Topic7 = "Europe",
  Topic8 = "Corona",
  Topic9 = "left/anti-war",
  Topic10 = "Twitter/politics_1",
  Topic11 = "Twitter/politics_2",
  Topic12 = "miscellaneous_2",
  Topic13 = "Twitter/politics_3",
  Topic14 = "right-wing extremism",
  Topic15 = "social/health"
)
```

```{r echo=FALSE}
topic_labels %>%
  matrix(dimnames = list(names(topic_labels))) %>%
  kable()
```

```{r include=FALSE}
doc_lengths <- lapply(data$documents[], length)
weights <- c()
i <- 1
while (i <= length(doc_lengths)) {
  weights[[i]] <- doc_lengths[[i]]/2
  i <- i + 1}
mean_weight <- mean(weights)
props_unweighted <- colMeans(mod_prev$theta[,1:K])
props_weighted <- colMeans((mod_prev$theta*weights/mean_weight)[, 1:K])

topic_labels_unlisted <- unlist(topic_labels, use.names = FALSE)

props_df <- data.frame(topic_labels_unlisted, props_unweighted, props_weighted) %>% reshape2::melt(id = "topic_labels_unlisted")
colnames(props_df) <- c("topic", "variable", "proportion")
```

Next, we identify two ways to calculate global topic proportions: either as the simple (unweighted) average of $\theta_d$ across all documents (i.e., as the average of MP-level proportions across all MPs); or by weighting each $\theta_d$ by the number of words in the respective documents, $N_d$. The table below shows all topics with their respective global proportions, for both weighting methodologies. We observe that for most topics, weighted and unweighted proportions are rather similar, but there are exceptions. In particular, the topics concerned with everyday political tweets have much higher unweighted than weighted frequencies; this makes sense, however, since such "diplomatic" tweets tend to be shorter than those which actually discuss a specific content.

```{r echo=FALSE}
ggplot(data = props_df, aes(x = topic, y = proportion, fill = variable)) + 
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("grey40","grey80")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1),
        axis.title.x = element_text(size = 12, face = "bold"),
        axis.title.y = element_text(size = 12, face = "bold"))
```

While labelling tells us which words best represent each topic - and thus, what each topic truly represents - it does not yet tell us to which extent individual topics are related to each other. In the graph below, we visualize the similarity of two topics, Topic 3 (`r topic_labels[3]`) and Topic 6 (`r topic_labels[6]`), in terms of their vocabulary usage. As suggested by the topic labels already, there is a significant overlap in vocabulary usage.

```{r echo=FALSE}
# vocabulary usage comparison for two topics
plot(mod_prev, type = "perspectives", topics = c(3, 6), n = 30)
```

More generally, we can evaluate the connectedness between different topics through the topic correlation matrix, which is simply based on the correlations between document-level topic proportions $\theta_d$. This is visualized in the graph below.

```{r echo=FALSE}
# global topic correlation
cormat <- cor(mod_prev$theta)
ggcorrplot::ggcorrplot(cormat) +
  scale_x_continuous(breaks = seq(1, 15, by = 1)) +
  scale_y_continuous(breaks = seq(1, 15, by = 1)) +
  labs(x = "topic number", y = "topic number") +
  theme_minimal() +
    theme(axis.title.x = element_text(size = 12, face = "bold"),
          axis.title.y = element_text(size = 12, face = "bold"))
```

Most topics are negatively correlated with each other, which does not come as a surprise, given the relatively low total number of topics, `r K`, and that topic proportions are "supplements": the higher one topic proportion, the lower the total of the others. Moreover, most topic correlations are rather weak in absolute size: the strongest negative  correlation (`r scales::percent(cormat[1,7], accuracy = 0.01)`)is the one between  topic  1 (`r topic_labels[[1]]`) and topic 7 (`r topic_labels[[1]]`), while the strongest positive correlation (`r scales::percent(cormat[3,6], accuracy = 0.01)`) is the one shown before, between (`r topic_labels[3]`) and (`r topic_labels[6]`).

We can also visualize these correlations using a network graph, where topics are connected whenever they are positively correlated. Most topics are only related to two other topics, while none are related to more than three. The only "isolated" topic is topic 8, `r topic_labels[8]`, which makes sense since it only entered the public sphere in early 2020, i.e., during the last months of our data collection period. In general, the relationships between the topics, as depicted below, are very much in line with their labelling.

```{r echo=FALSE}
set.seed(111)
mod_prev_corr <- topicCorr(mod_prev, method = "simple", cutoff = 0.00,
                           verbose = TRUE) # based on correlations between mod_prev$theta
vertex_sizes <- rep(20, K)
plot.topicCorr(mod_prev_corr, vlabels = topic_labels, vertex.label.cex = 1, vertex.size = vertex_sizes)
```

After this analysis of topics at a global level, in particular of their labeling and proportions, we now proceed to analyze metadata information (i.e., document-level covariates) and its impact on topic proportions. As mentioned before, the covariates we include are party, state (both categorical), date (smooth effect), percentage of immigrants, GDP per capita, unemployment rate, and the 2017 vote share (the last four as smooth effects, on an electoral-district level).

```{r echo=FALSE}
# factorize categorical variables, set CDU/CSU as reference category for variable "Partei"
data$meta$Partei <- data$meta$Partei %>%
  as.factor() %>%
  relevel(ref = 3)
data$meta$Bundesland <- as.factor(data$meta$Bundesland)

# prep <- stm::estimateEffect(
#   1:15 ~ s(t, df=20),
#   mod_prev,
#   #documents = data$documents,
#   metadata = data$meta,
#   uncertainty = "Global"
# )
# summary(prep, topics = 1)
# 
# par(mfrow=c(3,3))
# for (i in 1:9){
#   plot(prep, "t", method = "continuous", topics = i, 
#        main = paste0(mod_labels[i,], collapse = ", "), 
#        printlegend = F, xlab = "t")
# }
# par(mfrow=c(3,3))
# for (i in 1:9){
#   plot(prep, "Partei", method = "pointestimate", topics = i, labeltype = "custom",
#        custom.labels = c("CDU/CSU", "FDP", "Die Linke", "SPD", "Bündnis 90/Die Grünen", "AfD"), 
#        main = paste0(mod_labels[i,], collapse = ", "), 
#        printlegend = F, xlab = "Expected Topic Proportion")
# }

# ----------------------------------------------------------------------------------------------

library("betareg")
library("mvtnorm")

# ------------------------------- Create helper functions --------------------------------------

sigmoid <- function(x) exp(x)/(1+exp(x))

majority <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

sample_normal <- function(mod) {
  mu <- mod$coefficients$mean
  var <- mod$vcov[1:length(mu), 1:length(mu)]
  mvtnorm::rmvnorm(1, mean = mu, sigma = var)
}

sample_coefs_beta <- function(stmobj, formula, metadata, nsims = 25, seed = NULL){
  topic_n <- as.numeric(as.character(formula)[2])
  topic_nam <- paste0("Topic", topic_n)
  set.seed(seed)
  theta_sim <- do.call(rbind, stm::thetaPosterior(stmobj, nsims = nsims, type = "Global"))[,topic_n]
  theta_sim <- lapply(split(1:(length(theta_sim)), 1:nsims), 
                      function(i) setNames(data.frame(theta_sim[i]), topic_nam))
  f <- paste(topic_nam, "~", as.character(formula)[3])
  est_beta <- lapply(theta_sim, 
                     function(x) betareg::betareg(as.formula(f), data = cbind(x, metadata)))
  res <- lapply(est_beta, sample_normal)
  return(res)
}

sample_all_betas <- function(covar, metadata, nsims, topics = K, seed = NULL) {
  res <- vector(mode = "list", length = topics)
  for (topic in 1:topics) {
    outcome <- topic
    formula <- as.formula(paste(outcome, covar, sep = "~"))
    res[[topic]] <- sample_coefs_beta(mod_prev, formula, metadata = metadata, nsims, seed = seed)
  }
  return(res)
}

predict_props_beta <- function(beta_coefs, est_var, formula, metadata){
  dat <- metadata[, -which(names(metadata) == est_var)]
  dat <- lapply(dat, function(x) if(is.numeric(x)) median(x) else majority(x))
  if (is.numeric(metadata[,est_var])) {
    dat_fit <- data.frame(
      dat, fitvar = seq(min(metadata[,est_var]), max(metadata[,est_var]), length.out = 500)
    )
  } else {
    dat_fit <- data.frame(dat, fitvar = unique(metadata[,est_var]))
  }
  names(dat_fit) <- c(names(dat),est_var)
  f <- paste("~",as.character(formula)[3])
  xmat <- stm::makeDesignMatrix(as.formula(f), data$meta, dat_fit)
  fit_vals <- do.call(cbind, lapply(beta_coefs, function(x) sigmoid(xmat %*% t(x))))
  mu <- quanteda::rowMeans(fit_vals)
  ci <- apply(fit_vals, 1, function(x) quantile(x, probs = c(0.025, 0.975)))
  res <- data.frame(dat_fit[[est_var]], mu, ci[1,], ci[2,])
  names(res) <- c(est_var, "proportion", "ci_lower", "ci_upper")
  return(res)
}

# ----------------------------------------------------------------------------------------------

# ----------------------------- Actual Model Prediction ----------------------------------------

# covariates
covar <- "Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) + 
  s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)"
# formula always in form "i~var1+var2+...", where i = topic number
topic_number <- 1
formula <- as.formula(paste(topic_number, covar, sep = "~"))
# obtain list of nsims beta regression outputs
nsims <- 100

beta_coefs <- sample_coefs_beta(mod_prev, formula, data$meta, nsims = 25) # for topic = outcome only
# 
# set.seed(123)
# start.time <- Sys.time()
# all_betas <- sample_all_betas(covar = varlist, nsims = nsims, topics = K)
# end.time <- Sys.time()
# time.taken <- end.time - start.time
# time.taken
# saveRDS(all_betas, "./data/all_betas.rds")

# all_betas <- readRDS("./data/all_betas.rds")

# estimate effect for variable while other variables held as median/majority value
# preds <- predict_props_beta(beta_coefs, "t", formula, data$meta)

# example plots
# par(mfrow=c(3,3))
# for (v in c("t", "Partei", "Bundesland", "Struktur_4", "Struktur_22", "Struktur_42", "Struktur_54")) {
#   plot(predict_props_beta(beta_coefs, v, formula, data$meta)[,1:2], type = "l", col = "blue")
# }
```

```{r echo=FALSE}
library(grid)
library(gridExtra)
library(scales)

covar <- "Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) + 
  s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)"
varlist <- c(
  "t", "Partei", "Bundesland", "Struktur_4", "Struktur_22", "Struktur_42", "Struktur_54"
)
varlist_fullnames <- c(
  "Time", "Party", "Federal State", "Immigrants (%)", "GDP per capita", 
  "Unemployement Rate (%)", "vote share (%)"
)
```

We discuss the impact of covariates on topic proportions for topics 3 (`r topic_labels[[3]]`) and 4 (`r topic_labels[[4]]`), sub-dividing the analysis into smooth effects (time, immigration, GDP, and unemployment) and categorical variables (party and state). For smooth effects, it is important to recall that their borders are inherently unstable, which is why one should refrain from (over-)interpreting them.

By looking at the smooth effects for topic 3, we find that its proportion increases over time until the 25th month, corresponding to September 2019, decreasing sharply afterwards. However this sharp decline is to be taken with caution due to the instability of splines at the borders of the covariate domain. Note that the absolute changes in topic proportions over time for the `r topic_labels[[3]]` topic are rather small (around 4%). The effect of immigrants (as percentage of the total population) is negative across the entire domain, and rather steadily so. The impact of GDP per capita on topic 3 is unclear/constant, while unemployment rate show an overall positive effect. For all smooth effect except time, the proportion of the `r topic_labels[[3]]` topic due to covariate variation is about 8%.  

topic 3 smooth:
* time: increasing effect over time until sep2019, then falling (margins!); low overall proportions
* immigrants: steady (appr. linear) negative effect
* GDP per capita: no clear effect
* unemployment rate: increasing tendency
* topic proportions vary by a total of around 8%


We exemplify this using topics 3 (`r topic_labels[[3]]`) and 4 (`r topic_labels[[4]]`). First considering the smooth effects for topic 3, we take  `r topic_labels[[3]]` topic increases in importance over time until the 25th month, which corresponds to September 2019, decreasing sharply afterwards. However, since time is modelled as smooth effect, the margins are inherently unstable, which is why one should refraim from (over-)interpreting them.

```{r echo=FALSE}
## Topic 3: Green/Climate
formula_3 <- 3~Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) + 
  s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)
beta_coefs_3 <- sample_coefs_beta(mod_prev, formula_3, data$meta, nsims = 25)
preds_varlist_3 <- lapply(varlist, function(v) predict_props_beta(beta_coefs_3, v, formula_3, data$meta))
names(preds_varlist_3) <- varlist

### Continuous Plots
for(v in setdiff(varlist, c("Partei", "Bundesland"))){
  plot_nam <- paste0("plot_", v)
  assign(plot_nam, ggplot(preds_varlist_3[[v]], aes(!!as.symbol(v))) + 
           geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), fill = "grey70") +
           xlab(varlist_fullnames[varlist==v]) +
           ylab("Expected Topic Proportion") +
           geom_line(aes(y = proportion)) +
           scale_x_continuous(labels = scales::comma))
}
gridExtra::grid.arrange(
  plot_t, plot_Struktur_4, plot_Struktur_22, plot_Struktur_42, ncol=2, 
  top = grid::textGrob("Topic 3: Green/Climate", gp=grid::gpar(fontsize=16, fontface = "bold"))
)
```

topic 3 categorical:
* party: absolute effect sizes similar to those of the smooth effects (8%)
* green and liberal party with strongest effect

```{r echo=FALSE}
### Categorial Plots
(plot_party_3 <- ggplot(preds_varlist_3$Partei, aes(y=proportion, x = Partei)) +
  geom_crossbar(aes(ymax = ci_upper, ymin = ci_lower), fill = "grey70") +
  xlab("Party") +
  ylab("Expected Topic Proportion") +
  ggtitle("Topic 3: Green/Climate")+
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5)))
```

topic 4 smooth:
* most variables have a small impact (in absolute terms, 4% (vs 8% for topic 3))
* time: similar to effect on topic 3
* no clear effect for most of them

```{r echo=FALSE}
# ---------
## Topic 4: Social/Housing
formula_4 <- 4~Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) + 
  s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)
beta_coefs_4 <- sample_coefs_beta(mod_prev, formula_4, data$meta, nsims = 25)
preds_varlist_4 <- lapply(varlist, 
                          function(v) predict_props_beta(beta_coefs_4, v, formula_4, data$meta))
names(preds_varlist_4) <- varlist

### Continuous Plots
for(v in setdiff(varlist, c("Partei", "Bundesland"))){
  plot_nam <- paste0("plot_", v)
  assign(plot_nam, ggplot(preds_varlist_4[[v]], aes(!!as.symbol(v))) + 
    geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), fill = "grey70") +
    ylab("Expected Topic Proportion") +
    xlab(varlist_fullnames[varlist==v]) +
    geom_line(aes(y = proportion)) +
    scale_x_continuous(labels = scales::comma))
}
gridExtra::grid.arrange(
  plot_t, plot_Struktur_4, plot_Struktur_22, plot_Struktur_42, ncol=2, 
  top = grid::textGrob("Topic 4: Social/Housing", gp=grid::gpar(fontsize=16, fontface = "bold"))
)
```

topic 4 categorical:
* party: high topic prevalence for left parties, as was to be expected
* overall effect sizes a bit stronger than those of smooth effect for topic 4 (differece of 5%-6%)

```{r echo=FALSE}
### Categorial Plots
(plot_party_4 <- ggplot(preds_varlist_4$Partei, aes(y=proportion, x = Partei)) +
  geom_crossbar(aes(ymax = ci_upper, ymin = ci_lower), fill = "grey70") +
  xlab("Party") +
  ylab("Expected Topic Proportion") +
  ggtitle("Topic 4: Social/Housing")+
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5)))
```

topics 1, 3, and 4 for all parties:
* AfD very unimodel (more than 40% of the content tweeted by AfD party members is about right-wing national / immigration), other topics at around 5%
* for other parties: topic 1 at around 3-4%
* topics 3 and 4 as described above



```{r echo=FALSE}
## Topic 1: Right/Nationalist
formula_1 <- 1~Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) + 
  s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)
beta_coefs_1 <- sample_coefs_beta(mod_prev, formula_1, data$meta, nsims = 25)
preds_varlist_1 <- lapply(varlist, 
                          function(v) predict_props_beta(beta_coefs_1, v, formula_1, data$meta))
names(preds_varlist_1) <- varlist

# ---------

gridExtra::grid.arrange(plot_party_3, plot_party_4)

preds_varlist_1$Partei$Topic <- "Right/Nationalist"
preds_varlist_3$Partei$Topic <- "Green/Climate"
preds_varlist_4$Partei$Topic <- "Social/Housing"
party_data <- rbind(preds_varlist_1$Partei, preds_varlist_3$Partei, preds_varlist_4$Partei)
(plot_party <- ggplot(party_data, aes(y=proportion, x = Partei, fill = Topic)) +
    geom_col(position = "dodge") +
    scale_fill_manual(values=c("green", "blue", "red")) +
    xlab("Party") +
    ylab("Expected Topic Proportion") +
    ggtitle("Topic Proportions per Party")+
    theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5)))
```



### Literature



```{r echo=FALSE}

```
---
title: 'Data Collection, Preparation, and Preprocessing'
author: "Simon"
date: "May 2020"
output:
  pdf_document: default
  html_document: default
bibliography: bibliography.bib
biblio-style: myabbrvnat
link-citations: yes
linkcolor: blue
header-includes:
  - \usepackage{caption}
  - \usepackage{float}
  - \captionsetup{labelformat=empty}
  - \usepackage{multirow}
  - \usepackage{graphicx}
  - \usepackage{booktabs}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Preparation, include=FALSE}
# ----------------------------------------------------------------------------------------------
# ---------------------------------------- Preparation -----------------------------------------
# ----------------------------------------------------------------------------------------------

# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
  "huge", "knitr", "scales", "stm", "stringi", "tidyverse", "tm"
)
not_installed <- packages_required[!packages_required %in%
                                     installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
  lapply(
    not_installed,
    install.packages,
    repos = "http://cran.us.r-project.org",
    dependencies = TRUE,
    type = itype
  )
}
lapply(packages_required, library, character.only = TRUE)

```

### Introduction
* TBD
* define / translate / introduce German political concepts (Abgeordnete(r), Bundestag, Legislaturperiode, Wahlkreis, Ausschüsse, Parteienlandschaft)

### Theoretical Framework
* Introduce topic analysis vocabulary (documents, tokens, ...)

### Data

Data (2-3 Seiten):
* sentiment analysis: training corpus

```{r echo=FALSE, include=FALSE}
# load twitter data and aggregate on a per-user basis
topic <- readRDS("./data/topic.rds")

topic_user <- topic %>% 
  group_by(Name) %>% 
  mutate(Tweets_Dokument = paste(Tweets, collapse = ' ')) %>%
  summarize(
    Twitter_Username = max(Twitter_Username), 
    Tweets_Dokument = max(Tweets_Dokument),
    Anzahl_Follower = max(Anzahl_Follower)
  ) %>%
  ungroup()

# merge with personal and socioeconomic data
abg_df <- read_delim("../data/abg_df.csv", delim = ",") %>%
  rename(Twitter_Username = Twitter, Wahlkreis_Nr = `Wahlkreis-Nr.`)
se_df <- read_delim("../data/se_df.csv", delim = ",") %>% 
  rename(Wahlkreis_Nr = `Wahlkreis-Nr.`, "AfD Anteil" = "AFD Anteil") %>% 
  select(-Bundesland)

alldata <- topic %>% 
  inner_join(abg_df) %>% 
  inner_join(se_df, by = "Wahlkreis_Nr")

alldata <- alldata %>% 
  filter(Partei != "fraktionslos")

```

As a first step towards applying the STM to German political entities, we constructed a database with personal information about all German MPs. Using Python's BeautifulSoup web scraping tool as well as a selenium webdriver, we gathered data such as name, party, and electoral district from the [official parliament website](https://www.bundestag.de/abgeordnete) for all of the 709 members of the German parliament during its 19th election period, elected on September 24, 2017. (Footnote: MPs who resigned or passed away since this date were also listed on the website and thus included initially; they were manually excluded from further analysis.)

Since information on social media profiles was scarce and incomplete on the official parliament website, we scraped official party homepages for each of the six political parties represented in the current parliament. MPs who did not provide a Twitter account either on the official parliament website or on their party's official homepage were excluded. Using Python's tweepy library to access the official Twitter API, we scraped all tweets by German MPs from September 24, 2017 through April 24, 2020, i.e., during a total of 31 months. (Footnote: tweepy restricts the total number of retrievable tweets to 3,200. For those MPs with a larger number of tweets, the most recent 3,200 tweets are taken into account. However, this only affects two MPs.) This initially yielded `r nrow(topic)` tweets from a total of `r nrow(topic_user)` members of parliament.

To complement personal data, we also gathered socioeconomic data such as GDP per capita and unemployment rate as well as 2017 election results on an electoral district-level for all of the 299 electoral districts, from the [official electoral website](https://www.bundeswahlleiter.de). After removing  independent MPs as well as MPs without a specific electoral district assigned to them (for matchability with socioeconomic data), the final dataset counted `r nrow(prep)` MPs. The corresponding total number of tweets amounted to `r nrow(alldata)`. The table below shows total monthly tweet frequencies for our period of analysis, September 24, 2017 through April 24, 2020. As can be seen, tweet frequencies - though fluctuating - increase over time, peaking at almost 20,000 in March 2020.

```{r echo=FALSE, fig.align='center', fig.asp=.6, fig.width=10, warning=FALSE, out.width='82%'}

data_adj <- alldata %>% mutate(Jahr = lubridate::year(Datum), Monat = lubridate::month(Datum))
data_adj$date <- with(data_adj, sprintf("%d-%02d", Jahr, Monat))
ggplot(data_adj, aes(x = date)) + geom_histogram(stat = "count") +
  labs(x = "time", y = "# tweets")  +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1),
        axis.title.x = element_text(size = 12, face = "bold"),
        axis.title.y = element_text(size = 12, face = "bold"))
```

Next, data were grouped and tweets concatenated on a per-user level (thus aggregating tweets across the entire 31 months) as well as on a per-user per-month level, yielding a user-level and a (user-level) monthly dataset. This means that a document represents the concatenation of *all* of a single MP's tweets for the user-level dataset and a single MP's *monthly* tweets for the monthly dataset. This also means that MP-level metadata such as personal information and socioeconomic data (through the electoral district matching) can be used as document-level covariates. For the monthly dataset, the temporal component (year and month) constitutes an additional covariate. At this point, the data preparation was completed, thus marking the starting point of the preprocessing required for topic analysis, which is identical for both datasets.

We used the quanteda package within the R programming language for preprocessing. As a first step, we built a quanteda corpus from all documents, already transcribing German umlauts *ä/Ä*, *ö/Ö*, *ü/Ü* as well as German ligature *ß* as *ae/Ae*, *oe/Oe*, *ue/Ue*, and *ss* and removed hyphens. Next, we transformed the text data into a quanteda document-feature matrix (DFM), which essentially tokenizes texts, thereby convering all characters to lowercase. From the DFM, we removed an extensive list of German stopwords, using the [stopwords-iso GitHub repository](https://github.com/stopwords-iso/stopwords-iso), as well as English stopwords included in the quanteda package. Moreover, hashtags, usernames, quantities and units (e.g., *10kg* or *14.15uhr*), interjections (e.g., *aaahhh* or *ufff*), terms containing non-alphanumerical characters, meaningless word stumps (e.g., *innen* from the German female plural declension or *amp*, the remainder left after removing the ampersand sign, *&*) were removed. Terms with less than four characters and terms with a term frequency (overall number of occurrences) below five or with a document frequency (number of documents containing the word) below three are excluded. Finally, we manually removed over-frequent terms that would diminish the distinguishability of topics, such as *bundestag* or *polit*. 

We also performed word-stemming, which means cutting off word endings to remove discrepancies arising purely from declensions or conjugations - of particular importance for the German language. Due to the nature of the German language, the additional gains of lemmatization (which aims at identifying the base form of each word) would only be small as compared to the large increase in complexity, which is why we decided to use stemming only. Another issue when dealing with German language documents are compound words, which are sometimes hyphenated, basically leading to a distinction where semantically there is none. We address this issue by removing hyphens in the very beginning and converting all terms to lowercase, thus "gluing together" compound words; this way, terms like *Bundesregierung* and *Bundes-Regierung* are both transformed into *bundesregierung* (and, after stemming, into *bundesregier*). Finally, automatic segmentation techniques are not necesssary for the German language (@lucas2015computer).

```{r include=FALSE}
# load data
data <- readRDS("../data/preprocessed.rds")
```

As the result of preprocessing, one empty MP-level document was dropped, so that a total of `r nrow(data$meta)` MP-level documents were eventually analyzed, each one associated with `r ncol(data$meta)` covariates.

### Results (15-20 Seiten insgesamt):
- topic analysis:
	- main analyses w/ graphs, starting with main dataset
	- use party-specific and time-wise dataframes punctually to investigate specific relationships further
	- use input from Thurner & Heumann
	- go into detail regarding estimateEffect, topic prevalence, and topical content estimation (partially relate back to theoretical framework)

- sentiment analysis:
	- TBD

```{r include=FALSE}
# load data
data <- readRDS("../data/preprocessed.rds")
colnames_table <- read.csv(file = "../data/topic_colnames.csv")
data_corpus <- readRDS("../data/prep.rds")

# extract documents, vocabulary and metadata
docs <- data$documents
vocab <-  data$vocab
meta <- data$meta

# choose covariates and number of topics
prev_var <- c("Partei", "Bundesland")
outcome <- ""
prevalence <- as.formula(paste(outcome, paste(prev_var, collapse = "+"), sep = "~")) 

K <- 6

# fit model
mod_prev <- stm::stm(
  documents = docs,
  vocab = vocab,
  data = meta,
  K = K,
  prevalence = prevalence,
  # gamma.prior = 'L1',
  seed = 123,
  max.em.its = 200,
  init.type = "Spectral")
```

The first thing to do after fitting the model is to (visually) inspect the topics, in particular their best words. There are different metrics for evaluating which words are most representative of a topic. The STM comes with four such metrics: *highest probability*, *FREX*, *Lift*, and *Score*. The first one, *highest probability*, simply outputs for each topic those words in the topic-specific word vector, $\beta_{d,n}$, with the highest corpus frequency, i.e, with the highest absolute frequency across all documents. *FREX* takes into account not only how frequent but also how exclusive the words are: for a given topic, it is calculated by taking the harmonic mean of i) the word's rank by probability within specific topic (frequency) and ii) the topic's rank by the frequency of the word across all topics (exclusivity). Further information on the estimation of *FREX*, *Lift*, and *Score* can be found in @bischof2012summarizing, in the [lda package by Jonathan Chang (2015)](https://www.rdocumentation.org/packages/lda/versions/1.4.2/topics/lda-package), and in @taddy2012estimation, respectively.

The output below shows, for each topic, the seven top words for each of the four evaluation metrics.

```{r echo=FALSE}
# top words per topic (all topics)
topic_words <- labelTopics(mod_prev)
topic_words
```

Another way to visualize the most important words of a topic is via a word cloud. For a given topic (i.e., conditional upon a specific topic being chosen), it shows words weighted by their frequency. Judging at first sight, the topic seems to be about Europe and international cooperation.

```{r echo=FALSE}
# word cloud
topic_number <- 1
cloud(mod_prev, topic = topic_number, scale = c(2.0, 0.25))
```

To get a more thorough insight into the topic, we next look into actual documents, specifically into those which show the highest proportion for Topic 1.

```{r include=FALSE}
# # table of MAP topic proportions per document (for all topics)
# topic_props <- make.dt(
#   mod_prev, 
#   data$meta[
#     c("Name", "Partei", "Bundesland")]
# ) %>% cbind(docname = names(data$documents), .)
# 
# docs_number <- 5
# 
# ## (1) inspect most frequent words per topic
# topic_words$prob[topic_number,]
# 
# ## (2) evaluate most representative documents per topic
# topic_props %>%
#   arrange(desc(Topic1)) %>%
#   .[1:docs_number,c("Name", "Partei", paste0("Topic", topic_number))] %>%
#   left_join(select(data_corpus, "Tweets_Dokument")) %>%
#   select(c("Name", "Partei", paste0("Topic", topic_number), "Tweets_Dokument"))

## (3) assign label
topic_labels <- c("Europe",
                  "women/social",
                  "left",
                  "right/national",
                  "miscellaneous",
                  "climate/digital")
```

The documents confirm the first impression gained through top words and the word cloud: Topic 1 concerns Europe and international cooperation and we thus label it as *Europe*. We repeat this 4-step procedure (inspecting top words, evaluating the word cloud, reading through top documents, assigning a 1- or 2-word label) for all remaining topics, arriving at the following manual labels: *Europe*, *women/social*, *left*, *right/national*, *miscellaneous*, and *climate/digital*.

```{r include=FALSE}
doc_lengths <- lapply(docs[], length)
weights <- c()
i <- 1
while (i <= length(doc_lengths)) {
  weights[[i]] <- doc_lengths[[i]]/2
  i <- i + 1}
mean_weight <- mean(weights)
props_unweighted <- colMeans(mod_prev$theta[,1:K])
props_weighted <- colMeans((mod_prev$theta*weights/mean_weight)[, 1:K])

props_df <- data.frame(topic_labels, props_unweighted, props_weighted) %>% reshape2::melt(id = "topic_labels")
colnames(props_df) <- c("topic", "variable", "proportion")
```

We next identify two ways to calculate global topic proportions: either as the simple (unweighted) average of $\theta_d$ across all documents (i.e., as the average of MP-level proportions across all MPs); or by weighting each $\theta_d$ by the number of words in the respective documents, $N_d$. The table below shows all topics with their respective global proportions, for both weighting methodologies. For the miscellaneous Topic 5, the unweighted proportion, `r scales::percent(props_unweighted[5], accuracy = 0.01)` exceeds the weighted one, `r scales::percent(props_weighted[5], accuracy = 0.01)`, by more than
`r scales::percent(props_unweighted[5]-props_weighted[5], accuracy = NULL)`, whereas for all other topics, the proportions do not change too much. This indicates that the higher the absolute tweet volume of an MP, the higher the propensity to tweet about one of the more well-defined topics.

```{r echo=FALSE}
ggplot(data = props_df, aes(x = topic, y = proportion, fill = variable)) + 
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("grey40","grey80")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1),
        axis.title.x = element_text(size = 12, face = "bold"),
        axis.title.y = element_text(size = 12, face = "bold"))
```

While labelling tells us which words most represent each topic - and thus, what each topic truly represents - it does not yet tell us to which extent individual topics are related to each other. In the graph below, we visualize the similarity of two topics, Topic 2 (*women/social*) and Topic 3 (*left*), in terms of their vocabulary usage. As suggested by the top words already, there is a significant overlap in vocabulary usage. The main difference consists of the relevance of anti-war terminology in Topic 3.

```{r echo=FALSE}
# vocabulary usage comparison for two topics
plot(mod_prev, type = "perspectives", topics = c(2, 3), n = 30)
```

More generally, we can evaluate the connectedness between different topics through the topic correlation matrix, which is simply based on the correlations between document-level topic proportions $\theta_d$.

```{r echo=FALSE}
# global topic correlation
cormat <- cor(mod_prev$theta)
cormat
```

All topics are negatively correlated to each other, which does not come as a surprise, given the low total number of topics, `r K`, and that topic proportions are "supplements": the higher the proportion of one topic, the lower the proportions for the other ones. We can also visualize these correlations using a network graph, where topics are connected whenever their correlation exceeds -0.20.

```{r echo=FALSE}
mod_prev_corr <- topicCorr(mod_prev, method = "simple", cutoff = -0.20,
                           verbose = TRUE) # based on correlations between mod_prev$theta
plot.topicCorr(mod_prev_corr)
```

```{r echo=FALSE}

```

### Literature

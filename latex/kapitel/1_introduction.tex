\section{Introduction}
\label{Introduction}

The rise in popularity of social media has changed various aspects of private, public, and professional life over the last two decades. From a data-analytical point of view, this has led to an unprecedented increase in the supply of publicly available unstructured (text) data, ready to be analyzed. In fact, unstructured data makes up the lion's share of what is called big data (\citealp{gandomi2015beyond}). At the same time, advances in the field of machine learning, particularly in \textit{Natural Language Processing} (NLP), have created numerous new opportunities for the analysis of such large-scale unstructured texts.

A field which has been particularly impacted by the use of social media (and the information extracted from it) is politics. At least since the 2016 Brexit vote and US presidential election, politicians have come to recognize not only that social media presence is ever more important, but also how strong a message their social media behavior can transmit. Among social media networks, Twitter is of particular importance, since it allows for direct communication between politicians and voters - and even more so after the Facebook-Cambridge Analytica data breach in 2018. As a consequence, there has been increasing academic interest in text-based (intra- and inter-)party politics (e.g., \citealp{ceron2017intra, daniel2019static, grimmer2010bayesian, quinlan2018show}). Moreover, unstructured text and the insights generated from it can subsequently be used as input for a broad variety of tasks, ranging from election forecasts (e.g., \citealp{burnap2016140, jungherr2016twitter, tumasjan2010predicting}) to prediction of stock market movements (e.g., \citealp{nisar2018twitter}).

A key challenge in analyzing large amounts of unstructured text is to reduce dimensionality and classify pieces of text: either into previously determined categories (for instance, sentiments), which corresponds to a supervised learning problem; or by trying to discover latent thematic clusters that govern the content of the documents, which is now an instance of unsupervised learning (since the number and labeling of clusters is to be determined). In this paper, we use a mixture of both strategies - an unsupervised topic model followed by supervised regression analysis - and apply it to German politics. In doing so, our focus lies on statistical and methodological aspects of topic models, particularly on the relationship between topics and metadata, instead of specific (politological) hypothesis testing. We make four key contributions: first, we construct an extensive dataset which contains text data consisting of Twitter posts by German Members of Parliament (MPs) as well as a plenitude of personal data on an MP-level, socioeconomic and election data on an electoral-district level, and additional Twitter features; this dataset is then used in subsequent analysis but also provides a starting point for future research, particularly in political science. Second, we fit a \textit{Structural Topic Model} (STM) to our data to discover and describe latent topics within German parliamentarians' Twitter communication. Third, we critically discuss existing approaches to estimating topic-metadata relationships and develop new tools to improve model specification and account for uncertainty caused by topic interdependence. And fourth, we apply a train-test-split methodology to prevent overfitting and enable causal inference for document-level features.

We find that for most model specifications the majority of topics carry meaning, which can be regarded as a form of retrospective model validation. The fact that these topics are converted from mere word clusters into actually meaningful thematic clusters through manual labeling underlines the importance of human judgment in statistical topic modeling; this is in line with \cite{chang2009reading}, who show that solely focusing on quantitative metrics such as held-out likelihood does not guarantee meaningfulness of the latent space. As for document-level metadata, we discover some relevant associations between topic proportions and document features - particularly for the political parties, these relationships are in line with expectation. For continuous covariates such as unemployment and GDP we observe tendencies that are consistent across all modeling methodologies used - however, the high degree of uncertainty induced by the underlying generative process of the STM strongly outweighs these tendencies. The inclusion of a covariate to further model topical content (beyond its effect on topical prevalence) is found to reduce the meaningfulness of the latent space; furthermore, no intuitive natural candidate for the topical content variable exists in our case. Finally, we observe that (prevalence) covariate information and document information itself are used twice, for both model fitting \textit{and} effect estimation. In our case, the double usage of document-level metadata does have an impact on the estimated relationships, but the overall tendencies remain mostly unchanged. On the other hand, double usage of documents themselves induces a substantial degree of overfitting, which can be avoided by performing a train-test split; the resulting effects, in turn, are smaller but potentially allow for causal interpretation.

The remainder of this paper is organized as follows. Section \ref{Theoretical Framework} provides the theoretical foundation of topic modeling, in particular the "component models" of the STM which we use for the major part of our analysis, as well as a brief discussion of inference and parameter estimation. Section \ref{Data} describes the data collection process, the data itself, and the data preprocessing necessary for topic modeling. Section \ref{Model Selection and Global Characteristics} discusses model selection, labeling as well as global characteristics of the latent space. In section \ref{Metadata Analysis - Topical Prevalence and Content}, we include document-level metadata into the analysis, discussing the corresponding theory, methodological improvements, and results. Section \ref{Overfitting and Causal Inference - Alternative Modeling Strategies} deals with alternative modeling approaches and strategies for causal inference. Finally, section \ref{Conclusion} concludes.